{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))\n",
    "country=input(\"Enter the country: \")\n",
    "output_folder_path = input(\"Enter the output folder path: \")\n",
    "\n",
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR LOB EP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_tables = []\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table = table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Rename the aggregated column to Sum_Loss\n",
    "    grouped_table = grouped_table.rename_columns(['EventId', 'PeriodId', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "    # Append the grouped Table to the final_grouped_tables list\n",
    "    final_grouped_tables.append(grouped_table)\n",
    "\n",
    "# Concatenate all grouped tables\n",
    "final_table = pa.concat_tables(final_grouped_tables)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "final_grouped_table = final_table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table = final_grouped_table.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "# The Table is now ready for the next instructions\n",
    "dataframe_1 = sorted_final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LobName values to filter\n",
    "lob_names = ['AGR', 'AUTO', 'COM', 'IND', 'SPER', 'FRST', 'GLH']\n",
    "\n",
    "# Create a dictionary to store the filtered tables\n",
    "filtered_tables = {}\n",
    "\n",
    "# Filter the table based on LobName values\n",
    "for lob_name in lob_names:\n",
    "    filtered_table = dataframe_1.filter(pc.equal(dataframe_1['LobName'], lob_name))\n",
    "    if filtered_table.num_rows > 0:\n",
    "        filtered_tables[f'daf_{lob_name}'] = filtered_table\n",
    "\n",
    "# Access the filtered tables\n",
    "daf_AGR = filtered_tables.get('daf_AGR')\n",
    "daf_AUTO = filtered_tables.get('daf_AUTO')\n",
    "daf_COM = filtered_tables.get('daf_COM')\n",
    "daf_IND = filtered_tables.get('daf_IND')\n",
    "daf_SPER = filtered_tables.get('daf_SPER')\n",
    "daf_FRST = filtered_tables.get('daf_FRST')\n",
    "daf_GLH = filtered_tables.get('daf_GLH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_parquet(dataframe_1, parquet_file_path, speriod, samples):\n",
    "    # Convert pandas DataFrame to pyarrow Table if necessary\n",
    "    if not isinstance(dataframe_1, pa.Table):\n",
    "        dataframe_1 = pa.Table.from_pandas(dataframe_1)\n",
    "\n",
    "    # Initialize dataframe_2 by selecting PeriodId and max(Sum_Loss) grouped by PeriodId and LobName\n",
    "    dataframe_2 = dataframe_1.group_by(['PeriodId', 'LobName']).aggregate([('Loss', 'max')])\n",
    "    dataframe_2 = dataframe_2.rename_columns(['PeriodId', 'LobName', 'max_Loss'])\n",
    "\n",
    "    # Sort dataframe_2 by Max_Loss in descending order\n",
    "    dataframe_2 = dataframe_2.sort_by([('Max_Loss', 'descending')])\n",
    "\n",
    "    # Initialize dataframe_3 by selecting PeriodId and sum(Sum_Loss) grouped by PeriodId and LobName\n",
    "    dataframe_3 = dataframe_1.group_by(['PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "    dataframe_3 = dataframe_3.rename_columns(['PeriodId', 'LobName', 'sum_Loss'])\n",
    "\n",
    "    # Sort dataframe_3 by S_Sum_Loss in descending order\n",
    "    dataframe_3 = dataframe_3.sort_by([('sum_Loss', 'descending')])\n",
    "\n",
    "    # Calculate additional columns for dataframe_2\n",
    "    rate = 1 / (speriod * samples)\n",
    "    dataframe_2 = dataframe_2.append_column('rate', pa.array([rate] * dataframe_2.num_rows))\n",
    "    cumrate = pc.cumulative_sum(dataframe_2['rate'])\n",
    "    dataframe_2 = dataframe_2.append_column('cumrate', cumrate)\n",
    "    dataframe_2 = dataframe_2.append_column('RPs', pc.divide(1, cumrate))\n",
    "    max_loss_shifted = pc.shift(dataframe_2['max_Loss'], -1)\n",
    "    cumrate_shifted = pc.shift(cumrate, -1)\n",
    "    tce_oep_1 = pc.multiply(pc.multiply(pc.subtract(dataframe_2['max_Loss'], max_loss_shifted), pc.add(cumrate, cumrate_shifted)), 0.5)\n",
    "    dataframe_2 = dataframe_2.append_column('TCE_OEP_1', tce_oep_1)\n",
    "    tce_oep_2 = pc.multiply(pc.cumulative_sum(pc.shift(tce_oep_1, 1)), dataframe_2['RPs'])\n",
    "    dataframe_2 = dataframe_2.append_column('TCE_OEP_2', tce_oep_2)\n",
    "    dataframe_2 = dataframe_2.append_column('TCE_OEP_Final', pc.add(tce_oep_2, dataframe_2['max_Loss']))\n",
    "\n",
    "    # Calculate additional columns for dataframe_3\n",
    "    dataframe_3 = dataframe_3.append_column('rate', pa.array([rate] * dataframe_3.num_rows))\n",
    "    cumrate_3 = pc.cumulative_sum(dataframe_3['rate'])\n",
    "    dataframe_3 = dataframe_3.append_column('cumrate', cumrate_3)\n",
    "    dataframe_3 = dataframe_3.append_column('RPs', pc.divide(1, cumrate_3))\n",
    "    s_sum_loss_shifted = pc.shift(dataframe_3['sum_Loss'], -1)\n",
    "    cumrate_3_shifted = pc.shift(cumrate_3, -1)\n",
    "    tce_aep_1 = pc.multiply(pc.multiply(pc.subtract(dataframe_3['sum_Loss'], s_sum_loss_shifted), pc.add(cumrate_3, cumrate_3_shifted)), 0.5)\n",
    "    dataframe_3 = dataframe_3.append_column('TCE_AEP_1', tce_aep_1)\n",
    "    tce_aep_2 = pc.multiply(pc.cumulative_sum(pc.shift(tce_aep_1, 1)), dataframe_3['RPs'])\n",
    "    dataframe_3 = dataframe_3.append_column('TCE_AEP_2', tce_aep_2)\n",
    "    dataframe_3 = dataframe_3.append_column('TCE_AEP_Final', pc.add(tce_aep_2, dataframe_3['sum_Loss']))\n",
    "\n",
    "    # Define the list of RPs values to filter and convert them to float\n",
    "    rps_values = [float(x) for x in [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]]\n",
    "\n",
    "    # Initialize empty tables to store the filtered results\n",
    "    fdataframe_2 = pa.Table.from_arrays([], names=[])\n",
    "    fdataframe_3 = pa.Table.from_arrays([], names=[])\n",
    "\n",
    "    # Define the number of decimal places to round to\n",
    "    decimal_places = 8\n",
    "\n",
    "    # Loop through each value in rps_values and filter the Tables\n",
    "    for value in rps_values:\n",
    "        rounded_value = round(value, decimal_places)\n",
    "        fdataframe_2 = pa.concat_tables([fdataframe_2, dataframe_2.filter(pc.equal(pc.round(dataframe_2['RPs'], decimal_places), rounded_value))])\n",
    "        fdataframe_3 = pa.concat_tables([fdataframe_3, dataframe_3.filter(pc.equal(pc.round(dataframe_3['RPs'], decimal_places), rounded_value))])\n",
    "\n",
    "    fdataframe_3 = fdataframe_3.rename_columns(['PeriodId', 'LobName', 'S_Sum_Loss', 'rate', 'cumrate', 'RPs', 'TCE_AEP_1', 'TCE_AEP_2', 'TCE_AEP_Final'])\n",
    "    fdataframe_2 = fdataframe_2.rename_columns(['PeriodId', 'LobName', 'Max_Loss', 'rate', 'cumrate', 'RPs', 'TCE_OEP_1', 'TCE_OEP_2', 'TCE_OEP_Final'])\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': \"1\",\n",
    "        'AUTO': \"2\",\n",
    "        'COM': \"3\",\n",
    "        'IND': \"4\",\n",
    "        'SPER': \"5\",\n",
    "        'FRST': \"6\",\n",
    "        'GLH': \"7\"\n",
    "    }\n",
    "\n",
    "    # Add the LobId column to fdataframe_2\n",
    "    fdataframe_2 = fdataframe_2.append_column('LobId', pc.index_in(fdataframe_2['LobName'], list(lobname_to_lobid.keys())).cast(pa.int32()))\n",
    "\n",
    "    # Add the LobId column to fdataframe_3\n",
    "    fdataframe_3 = fdataframe_3.append_column('LobId', pc.index_in(fdataframe_3['LobName'], list(lobname_to_lobid.keys())).cast(pa.int32()))\n",
    "\n",
    "    # Define the columns to be used in the new Table for fdataframe_3\n",
    "    columns_to_keep_3 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_3 = ['S_Sum_Loss', 'TCE_AEP_Final']\n",
    "\n",
    "    # Melt fdataframe_3 to reshape it\n",
    "    melted_df_3 = pa.Table.from_pandas(fdataframe_3.to_pandas().melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "                                    var_name='EPType', value_name='Loss'))\n",
    "\n",
    "    # Rename columns to match the desired output\n",
    "    melted_df_3 = melted_df_3.rename_columns(['ReturnPeriod', 'LobId', 'LobName', 'EPType', 'Loss'])\n",
    "\n",
    "    # Define the columns to be used in the new Table for fdataframe_2\n",
    "    columns_to_keep_2 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_2 = ['max_Loss', 'TCE_OEP_Final']\n",
    "\n",
    "    # Melt fdataframe_2 to reshape it\n",
    "    melted_df_2 = pa.Table.from_pandas(fdataframe_2.to_pandas().melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "                                    var_name='EPType', value_name='Loss'))\n",
    "\n",
    "    # Rename columns to match the desired output\n",
    "    melted_df_2 = melted_df_2.rename_columns(['ReturnPeriod', 'LobId', 'LobName', 'EPType', 'Loss'])\n",
    "\n",
    "    # Concatenate the two Tables\n",
    "    final_df_EP_LOB_GU = pa.concat_tables([melted_df_2, melted_df_3])\n",
    "\n",
    "    # Define the new order for EPType\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "\n",
    "    # Update the EPType column to the new order\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.append_column('EPType', pa.array(new_ep_type_order))\n",
    "\n",
    "    # Sort the Table by EPType and then by ReturnPeriod in descending order within each EPType\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_by([('EPType', 'ascending'), ('ReturnPeriod', 'descending')])\n",
    "\n",
    "    # Save final_df as a Parquet file\n",
    "    pq.write_table(final_df_EP_LOB_GU, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_path_1=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_0.parquet')\n",
    "\n",
    "pq_file_path_2=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_1.parquet')\n",
    "\n",
    "pq_file_path_3=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_2.parquet')\n",
    "\n",
    "pq_file_path_4=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_3.parquet')\n",
    "\n",
    "pq_file_path_5=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_4.parquet')\n",
    "\n",
    "pq_file_path_6=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_5.parquet')\n",
    "\n",
    "pq_file_path_7=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "No match for FieldRef.Name(Loss) in EventId: int64 not null\nPeriodId: int64 not null\nLobName: string\nSum_Loss_sum: double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\Complete_pyarrow.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     process_and_save_parquet(daf_AGR, pq_file_path_1, speriod, samples)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\Complete_pyarrow.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     dataframe_1 \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_pandas(dataframe_1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Initialize dataframe_2 by selecting PeriodId and max(Sum_Loss) grouped by PeriodId and LobName\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m dataframe_2 \u001b[39m=\u001b[39m dataframe_1\u001b[39m.\u001b[39;49mgroup_by([\u001b[39m'\u001b[39;49m\u001b[39mPeriodId\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mLobName\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49maggregate([(\u001b[39m'\u001b[39;49m\u001b[39mLoss\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m)])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataframe_2 \u001b[39m=\u001b[39m dataframe_2\u001b[39m.\u001b[39mrename_columns([\u001b[39m'\u001b[39m\u001b[39mPeriodId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmax_Loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/Complete_pyarrow.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Sort dataframe_2 by Max_Loss in descending order\u001b[39;00m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\table.pxi:6509\u001b[0m, in \u001b[0;36mpyarrow.lib.TableGroupBy.aggregate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\acero.py:403\u001b[0m, in \u001b[0;36m_group_by\u001b[1;34m(table, aggregates, keys, use_threads)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_group_by\u001b[39m(table, aggregates, keys, use_threads\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    399\u001b[0m     decl \u001b[39m=\u001b[39m Declaration\u001b[39m.\u001b[39mfrom_sequence([\n\u001b[0;32m    400\u001b[0m         Declaration(\u001b[39m\"\u001b[39m\u001b[39mtable_source\u001b[39m\u001b[39m\"\u001b[39m, TableSourceNodeOptions(table)),\n\u001b[0;32m    401\u001b[0m         Declaration(\u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m, AggregateNodeOptions(aggregates, keys\u001b[39m=\u001b[39mkeys))\n\u001b[0;32m    402\u001b[0m     ])\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m decl\u001b[39m.\u001b[39;49mto_table(use_threads\u001b[39m=\u001b[39;49muse_threads)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\_acero.pyx:590\u001b[0m, in \u001b[0;36mpyarrow._acero.Declaration.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: No match for FieldRef.Name(Loss) in EventId: int64 not null\nPeriodId: int64 not null\nLobName: string\nSum_Loss_sum: double"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    process_and_save_parquet(daf_AGR, pq_file_path_1, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_AUTO, pq_file_path_2, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_COM, pq_file_path_3, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_IND, pq_file_path_4, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_SPER, pq_file_path_5, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_FRST, pq_file_path_6, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_GLH, pq_file_path_7, speriod, samples)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
