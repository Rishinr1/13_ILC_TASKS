{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_3\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "flush_cache()\n",
    "speriod=50000\n",
    "samples=5\n",
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "output_folder_path = \"D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_3\"\n",
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for LOB AUTO portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'LobName','LobId','EventId', 'PeriodId','Weight', 'EventDate','LossDate','Region','Peril'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import pyarrow.compute as pc\n",
    "# import pyarrow.dataset as ds\n",
    "# import os\n",
    "\n",
    "# processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "# os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# # Initialize an empty list to store the results\n",
    "# final_grouped_table_1 = []\n",
    "\n",
    "# # Define the chunk size\n",
    "# chunk_size = 4_000_000  # 4 million rows per chunk\n",
    "\n",
    "# # Process each Parquet file individually\n",
    "# for file in parquet_files:\n",
    "#     # Create a dataset from the Parquet file\n",
    "#     dataset = ds.dataset(file, format=\"parquet\")\n",
    "\n",
    "#     # Read the dataset in chunks\n",
    "#     for batch in dataset.to_batches(batch_size=chunk_size):\n",
    "#         table = pa.Table.from_batches([batch])\n",
    "        \n",
    "#         # Filter the table\n",
    "#         table = table.filter(pc.equal(table['LobName'], 'AUTO'))\n",
    "\n",
    "#         # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and EventDate\n",
    "#         grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "#         grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "        \n",
    "#         # Write intermediate results to disk\n",
    "#         pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "# # Read all intermediate files and concatenate them\n",
    "# intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "# final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "# final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "# # Perform final grouping and sorting\n",
    "# f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "# sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# # Convert to pandas DataFrame\n",
    "# dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "\n",
    "# # Delete all non-concatenated files\n",
    "# for f in intermediate_files_1:\n",
    "#     os.remove(f)\n",
    "\n",
    "# print(dataframe_1_oep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_table_1 = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, LobName, and LocationId\n",
    "    table = table.filter(pc.equal(table['LobName'], 'AUTO'))\n",
    "\n",
    "    grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate','Sum_Loss'])\n",
    "   \n",
    "    # Write intermediate results to disk\n",
    "    pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "# Read all intermediate files and concatenate them\n",
    "intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "\n",
    "\n",
    "\n",
    "# Delete all non-concatenated files\n",
    "for f in intermediate_files_1 :\n",
    "    os.remove(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import os\n",
    "\n",
    "processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_table_1 = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, LobName, and LocationId\n",
    "    table = table.filter(pc.equal(table['LobName'], 'AUTO'))\n",
    "\n",
    "    grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "   \n",
    "    # Write intermediate results to disk\n",
    "    pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "# Read all intermediate files and concatenate them\n",
    "intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "\n",
    "# Delete all non-concatenated files\n",
    "for f in intermediate_files_1:\n",
    "    os.remove(f)\n",
    "\n",
    "print(dataframe_1_oep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>LobName</th>\n",
       "      <th>Sum_Loss_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53895970</td>\n",
       "      <td>207557</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.250859e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53891598</td>\n",
       "      <td>149148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.197889e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53893301</td>\n",
       "      <td>4312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.160561e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53893301</td>\n",
       "      <td>154312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.092983e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53891598</td>\n",
       "      <td>199148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.993543e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374263</th>\n",
       "      <td>53876702</td>\n",
       "      <td>165828</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>1.870107e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374264</th>\n",
       "      <td>53865125</td>\n",
       "      <td>130081</td>\n",
       "      <td>AGR</td>\n",
       "      <td>1.669423e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374265</th>\n",
       "      <td>53881107</td>\n",
       "      <td>82560</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>8.474226e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374266</th>\n",
       "      <td>53872002</td>\n",
       "      <td>212031</td>\n",
       "      <td>AGR</td>\n",
       "      <td>6.278728e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374267</th>\n",
       "      <td>53885692</td>\n",
       "      <td>249127</td>\n",
       "      <td>SPER</td>\n",
       "      <td>3.690754e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7374268 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EventId  PeriodId LobName  Sum_Loss_sum\n",
       "0        53895970    207557    SPER  2.250859e+10\n",
       "1        53891598    149148    SPER  2.197889e+10\n",
       "2        53893301      4312    SPER  2.160561e+10\n",
       "3        53893301    154312    SPER  2.092983e+10\n",
       "4        53891598    199148    SPER  1.993543e+10\n",
       "...           ...       ...     ...           ...\n",
       "7374263  53876702    165828    AUTO  1.870107e-03\n",
       "7374264  53865125    130081     AGR  1.669423e-03\n",
       "7374265  53881107     82560    AUTO  8.474226e-04\n",
       "7374266  53872002    212031     AGR  6.278728e-04\n",
       "7374267  53885692    249127    SPER  3.690754e-04\n",
       "\n",
       "[7374268 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_1_aep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>Sum_Loss_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53891598</td>\n",
       "      <td>149148</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>1.654085e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53891598</td>\n",
       "      <td>199148</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>1.509087e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53895970</td>\n",
       "      <td>207557</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>1.359457e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53893301</td>\n",
       "      <td>4312</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>1.319517e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53893301</td>\n",
       "      <td>154312</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>1.253157e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550192</th>\n",
       "      <td>53856304</td>\n",
       "      <td>53560</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>3.063393e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550193</th>\n",
       "      <td>53855161</td>\n",
       "      <td>89818</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>2.959355e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550194</th>\n",
       "      <td>53867896</td>\n",
       "      <td>132912</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>2.819001e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550195</th>\n",
       "      <td>53876702</td>\n",
       "      <td>165828</td>\n",
       "      <td>2020-10-21</td>\n",
       "      <td>1.870107e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550196</th>\n",
       "      <td>53881107</td>\n",
       "      <td>82560</td>\n",
       "      <td>2020-10-25</td>\n",
       "      <td>8.474226e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1550197 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EventId  PeriodId  EventDate  Sum_Loss_sum\n",
       "0        53891598    149148 2020-05-13  1.654085e+09\n",
       "1        53891598    199148 2020-05-13  1.509087e+09\n",
       "2        53895970    207557 2020-05-30  1.359457e+09\n",
       "3        53893301      4312 2020-11-10  1.319517e+09\n",
       "4        53893301    154312 2020-11-10  1.253157e+09\n",
       "...           ...       ...        ...           ...\n",
       "1550192  53856304     53560 2020-03-26  3.063393e-03\n",
       "1550193  53855161     89818 2020-02-09  2.959355e-03\n",
       "1550194  53867896    132912 2020-02-29  2.819001e-03\n",
       "1550195  53876702    165828 2020-10-21  1.870107e-03\n",
       "1550196  53881107     82560 2020-10-25  8.474226e-04\n",
       "\n",
       "[1550197 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_1_oep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_2 = dataframe_1_oep.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "dataframe_3 = dataframe_1_oep.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>Max_Loss</th>\n",
       "      <th>rate</th>\n",
       "      <th>cumrate</th>\n",
       "      <th>RPs</th>\n",
       "      <th>TCE_OEP_1</th>\n",
       "      <th>TCE_OEP_2</th>\n",
       "      <th>TCE_OEP_Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149148</td>\n",
       "      <td>1.654085e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>869.991566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199148</td>\n",
       "      <td>1.509087e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>125000.000000</td>\n",
       "      <td>1496.296836</td>\n",
       "      <td>1.087489e+08</td>\n",
       "      <td>1.617836e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207557</td>\n",
       "      <td>1.359457e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>83333.333333</td>\n",
       "      <td>559.156488</td>\n",
       "      <td>1.971907e+08</td>\n",
       "      <td>1.556648e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4312</td>\n",
       "      <td>1.319517e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>62500.000000</td>\n",
       "      <td>1194.484886</td>\n",
       "      <td>1.828403e+08</td>\n",
       "      <td>1.502357e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154312</td>\n",
       "      <td>1.253157e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1155.475354</td>\n",
       "      <td>2.059965e+08</td>\n",
       "      <td>1.459153e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247708</th>\n",
       "      <td>39961</td>\n",
       "      <td>9.574042e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990836</td>\n",
       "      <td>1.009249</td>\n",
       "      <td>0.769077</td>\n",
       "      <td>9.832831e+06</td>\n",
       "      <td>9.832832e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247709</th>\n",
       "      <td>162941</td>\n",
       "      <td>1.812157e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990840</td>\n",
       "      <td>1.009245</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>9.832792e+06</td>\n",
       "      <td>9.832792e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247710</th>\n",
       "      <td>241283</td>\n",
       "      <td>1.777667e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990844</td>\n",
       "      <td>1.009241</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>9.832752e+06</td>\n",
       "      <td>9.832752e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247711</th>\n",
       "      <td>71005</td>\n",
       "      <td>1.736658e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990848</td>\n",
       "      <td>1.009237</td>\n",
       "      <td>0.046673</td>\n",
       "      <td>9.832712e+06</td>\n",
       "      <td>9.832712e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247712</th>\n",
       "      <td>139961</td>\n",
       "      <td>1.265618e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990852</td>\n",
       "      <td>1.009232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.832673e+06</td>\n",
       "      <td>9.832673e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247713 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PeriodId      Max_Loss      rate   cumrate            RPs  \\\n",
       "0         149148  1.654085e+09  0.000004  0.000004  250000.000000   \n",
       "1         199148  1.509087e+09  0.000004  0.000008  125000.000000   \n",
       "2         207557  1.359457e+09  0.000004  0.000012   83333.333333   \n",
       "3           4312  1.319517e+09  0.000004  0.000016   62500.000000   \n",
       "4         154312  1.253157e+09  0.000004  0.000020   50000.000000   \n",
       "...          ...           ...       ...       ...            ...   \n",
       "247708     39961  9.574042e-01  0.000004  0.990836       1.009249   \n",
       "247709    162941  1.812157e-01  0.000004  0.990840       1.009245   \n",
       "247710    241283  1.777667e-01  0.000004  0.990844       1.009241   \n",
       "247711     71005  1.736658e-01  0.000004  0.990848       1.009237   \n",
       "247712    139961  1.265618e-01  0.000004  0.990852       1.009232   \n",
       "\n",
       "          TCE_OEP_1     TCE_OEP_2  TCE_OEP_Final  \n",
       "0        869.991566           NaN            NaN  \n",
       "1       1496.296836  1.087489e+08   1.617836e+09  \n",
       "2        559.156488  1.971907e+08   1.556648e+09  \n",
       "3       1194.484886  1.828403e+08   1.502357e+09  \n",
       "4       1155.475354  2.059965e+08   1.459153e+09  \n",
       "...             ...           ...            ...  \n",
       "247708     0.769077  9.832831e+06   9.832832e+06  \n",
       "247709     0.003417  9.832792e+06   9.832792e+06  \n",
       "247710     0.004063  9.832752e+06   9.832752e+06  \n",
       "247711     0.046673  9.832712e+06   9.832712e+06  \n",
       "247712          NaN  9.832673e+06   9.832673e+06  \n",
       "\n",
       "[247713 rows x 8 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>Max_Loss</th>\n",
       "      <th>rate</th>\n",
       "      <th>cumrate</th>\n",
       "      <th>RPs</th>\n",
       "      <th>TCE_OEP_1</th>\n",
       "      <th>TCE_OEP_2</th>\n",
       "      <th>TCE_OEP_Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124999</th>\n",
       "      <td>7383</td>\n",
       "      <td>1.454844e+06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.946862</td>\n",
       "      <td>1.750499e+07</td>\n",
       "      <td>1.895984e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PeriodId      Max_Loss      rate  cumrate  RPs  TCE_OEP_1  \\\n",
       "124999      7383  1.454844e+06  0.000004      0.5  2.0   6.946862   \n",
       "\n",
       "           TCE_OEP_2  TCE_OEP_Final  \n",
       "124999  1.750499e+07   1.895984e+07  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_2[dataframe_2[\"PeriodId\"]==7383]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>S_Sum_Loss</th>\n",
       "      <th>rate</th>\n",
       "      <th>cumrate</th>\n",
       "      <th>RPs</th>\n",
       "      <th>TCE_AEP_1</th>\n",
       "      <th>TCE_AEP_2</th>\n",
       "      <th>TCE_AEP_Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149148</td>\n",
       "      <td>1.660418e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>848.360420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.320836e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199148</td>\n",
       "      <td>1.519025e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>125000.000000</td>\n",
       "      <td>1590.599828</td>\n",
       "      <td>1.060451e+08</td>\n",
       "      <td>3.038050e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207557</td>\n",
       "      <td>1.359965e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>83333.333333</td>\n",
       "      <td>110.572131</td>\n",
       "      <td>2.032467e+08</td>\n",
       "      <td>2.719930e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4312</td>\n",
       "      <td>1.352067e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>62500.000000</td>\n",
       "      <td>1255.639742</td>\n",
       "      <td>1.593458e+08</td>\n",
       "      <td>2.704134e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154312</td>\n",
       "      <td>1.282309e+09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1378.466997</td>\n",
       "      <td>1.902586e+08</td>\n",
       "      <td>2.564618e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247708</th>\n",
       "      <td>39961</td>\n",
       "      <td>9.574042e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990836</td>\n",
       "      <td>1.009249</td>\n",
       "      <td>0.769077</td>\n",
       "      <td>1.245118e+07</td>\n",
       "      <td>1.914808e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247709</th>\n",
       "      <td>162941</td>\n",
       "      <td>1.812157e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990840</td>\n",
       "      <td>1.009245</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>1.245113e+07</td>\n",
       "      <td>3.624314e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247710</th>\n",
       "      <td>241283</td>\n",
       "      <td>1.777667e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990844</td>\n",
       "      <td>1.009241</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>1.245108e+07</td>\n",
       "      <td>3.555334e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247711</th>\n",
       "      <td>71005</td>\n",
       "      <td>1.736658e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990848</td>\n",
       "      <td>1.009237</td>\n",
       "      <td>0.046673</td>\n",
       "      <td>1.245103e+07</td>\n",
       "      <td>3.473316e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247712</th>\n",
       "      <td>139961</td>\n",
       "      <td>1.265618e-01</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.990852</td>\n",
       "      <td>1.009232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.245098e+07</td>\n",
       "      <td>2.531237e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247713 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PeriodId    S_Sum_Loss      rate   cumrate            RPs  \\\n",
       "0         149148  1.660418e+09  0.000004  0.000004  250000.000000   \n",
       "1         199148  1.519025e+09  0.000004  0.000008  125000.000000   \n",
       "2         207557  1.359965e+09  0.000004  0.000012   83333.333333   \n",
       "3           4312  1.352067e+09  0.000004  0.000016   62500.000000   \n",
       "4         154312  1.282309e+09  0.000004  0.000020   50000.000000   \n",
       "...          ...           ...       ...       ...            ...   \n",
       "247708     39961  9.574042e-01  0.000004  0.990836       1.009249   \n",
       "247709    162941  1.812157e-01  0.000004  0.990840       1.009245   \n",
       "247710    241283  1.777667e-01  0.000004  0.990844       1.009241   \n",
       "247711     71005  1.736658e-01  0.000004  0.990848       1.009237   \n",
       "247712    139961  1.265618e-01  0.000004  0.990852       1.009232   \n",
       "\n",
       "          TCE_AEP_1     TCE_AEP_2  TCE_AEP_Final  \n",
       "0        848.360420           NaN   3.320836e+09  \n",
       "1       1590.599828  1.060451e+08   3.038050e+09  \n",
       "2        110.572131  2.032467e+08   2.719930e+09  \n",
       "3       1255.639742  1.593458e+08   2.704134e+09  \n",
       "4       1378.466997  1.902586e+08   2.564618e+09  \n",
       "...             ...           ...            ...  \n",
       "247708     0.769077  1.245118e+07   1.914808e+00  \n",
       "247709     0.003417  1.245113e+07   3.624314e-01  \n",
       "247710     0.004063  1.245108e+07   3.555334e-01  \n",
       "247711     0.046673  1.245103e+07   3.473316e-01  \n",
       "247712          NaN  1.245098e+07   2.531237e-01  \n",
       "\n",
       "[247713 rows x 8 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_2.to_csv(r\"D:\\RISHIN\\Rough\\Calculated_OEP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_2.to_csv(r\"D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_3\\LOB_AUTO_SECOND.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\POrtfolio_AUTO.parquet\n"
     ]
    }
   ],
   "source": [
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "fdataframe_2 = pd.DataFrame()\n",
    "fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "\n",
    "for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "\n",
    "fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "columns_to_keep_2 = ['RPs']\n",
    "columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2,var_name='EPType', value_name='Loss')\n",
    "melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "columns_to_keep_3 = ['RPs']\n",
    "columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3,var_name='EPType', value_name='Loss')\n",
    "melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "\n",
    "final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Define the schema to match the required Parquet file schema\n",
    "schema = pa.schema([\n",
    "    pa.field('EPType', pa.string(), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "       \n",
    "])\n",
    "parquet_file_path=r\"D:\\RISHIN\\Rough\\POrtfolio_AUTO.parquet\"\n",
    "# Convert DataFrame to Arrow Table with the specified schema\n",
    "table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "# Save to Parquet\n",
    "pq.write_table(table, parquet_file_path)\n",
    "\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_EP_LOB_GU.to_csv(r\"D:\\RISHIN\\Rough\\POrtfolio_AUTO.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPType</th>\n",
       "      <th>Loss</th>\n",
       "      <th>ReturnPeriod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OEP</td>\n",
       "      <td>8.639885e+08</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OEP</td>\n",
       "      <td>6.749918e+08</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OEP</td>\n",
       "      <td>3.778001e+08</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OEP</td>\n",
       "      <td>2.847455e+08</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OEP</td>\n",
       "      <td>2.064693e+08</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OEP</td>\n",
       "      <td>1.867783e+08</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OEP</td>\n",
       "      <td>1.331685e+08</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OEP</td>\n",
       "      <td>8.957940e+07</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OEP</td>\n",
       "      <td>5.637969e+07</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OEP</td>\n",
       "      <td>2.190680e+07</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OEP</td>\n",
       "      <td>8.794311e+06</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OEP</td>\n",
       "      <td>1.454844e+06</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AEP</td>\n",
       "      <td>9.009420e+08</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AEP</td>\n",
       "      <td>6.933070e+08</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AEP</td>\n",
       "      <td>4.042571e+08</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AEP</td>\n",
       "      <td>3.133708e+08</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AEP</td>\n",
       "      <td>2.347901e+08</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AEP</td>\n",
       "      <td>2.138238e+08</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AEP</td>\n",
       "      <td>1.565768e+08</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AEP</td>\n",
       "      <td>1.089063e+08</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AEP</td>\n",
       "      <td>6.947576e+07</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AEP</td>\n",
       "      <td>2.888414e+07</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AEP</td>\n",
       "      <td>1.235320e+07</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AEP</td>\n",
       "      <td>2.439481e+06</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>1.108640e+09</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>9.229325e+08</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>5.629396e+08</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>4.436586e+08</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>3.412711e+08</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>3.123921e+08</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>2.344683e+08</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>1.713539e+08</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>1.209697e+08</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>6.949271e+07</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>4.170271e+07</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>TCE-OEP</td>\n",
       "      <td>1.895984e+07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>1.134833e+09</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>9.529348e+08</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>5.894961e+08</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>4.717832e+08</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>3.705608e+08</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>3.413128e+08</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>2.611342e+08</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>1.951568e+08</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>1.405628e+08</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>8.298603e+07</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>5.090346e+07</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TCE-AEP</td>\n",
       "      <td>2.380638e+07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EPType          Loss  ReturnPeriod\n",
       "0       OEP  8.639885e+08       10000.0\n",
       "1       OEP  6.749918e+08        5000.0\n",
       "2       OEP  3.778001e+08        1000.0\n",
       "3       OEP  2.847455e+08         500.0\n",
       "4       OEP  2.064693e+08         250.0\n",
       "5       OEP  1.867783e+08         200.0\n",
       "6       OEP  1.331685e+08         100.0\n",
       "7       OEP  8.957940e+07          50.0\n",
       "8       OEP  5.637969e+07          25.0\n",
       "9       OEP  2.190680e+07          10.0\n",
       "10      OEP  8.794311e+06           5.0\n",
       "11      OEP  1.454844e+06           2.0\n",
       "12      AEP  9.009420e+08       10000.0\n",
       "13      AEP  6.933070e+08        5000.0\n",
       "14      AEP  4.042571e+08        1000.0\n",
       "15      AEP  3.133708e+08         500.0\n",
       "16      AEP  2.347901e+08         250.0\n",
       "17      AEP  2.138238e+08         200.0\n",
       "18      AEP  1.565768e+08         100.0\n",
       "19      AEP  1.089063e+08          50.0\n",
       "20      AEP  6.947576e+07          25.0\n",
       "21      AEP  2.888414e+07          10.0\n",
       "22      AEP  1.235320e+07           5.0\n",
       "23      AEP  2.439481e+06           2.0\n",
       "24  TCE-OEP  1.108640e+09       10000.0\n",
       "25  TCE-OEP  9.229325e+08        5000.0\n",
       "26  TCE-OEP  5.629396e+08        1000.0\n",
       "27  TCE-OEP  4.436586e+08         500.0\n",
       "28  TCE-OEP  3.412711e+08         250.0\n",
       "29  TCE-OEP  3.123921e+08         200.0\n",
       "30  TCE-OEP  2.344683e+08         100.0\n",
       "31  TCE-OEP  1.713539e+08          50.0\n",
       "32  TCE-OEP  1.209697e+08          25.0\n",
       "33  TCE-OEP  6.949271e+07          10.0\n",
       "34  TCE-OEP  4.170271e+07           5.0\n",
       "35  TCE-OEP  1.895984e+07           2.0\n",
       "36  TCE-AEP  1.134833e+09       10000.0\n",
       "37  TCE-AEP  9.529348e+08        5000.0\n",
       "38  TCE-AEP  5.894961e+08        1000.0\n",
       "39  TCE-AEP  4.717832e+08         500.0\n",
       "40  TCE-AEP  3.705608e+08         250.0\n",
       "41  TCE-AEP  3.413128e+08         200.0\n",
       "42  TCE-AEP  2.611342e+08         100.0\n",
       "43  TCE-AEP  1.951568e+08          50.0\n",
       "44  TCE-AEP  1.405628e+08          25.0\n",
       "45  TCE-AEP  8.298603e+07          10.0\n",
       "46  TCE-AEP  5.090346e+07           5.0\n",
       "47  TCE-AEP  2.380638e+07           2.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_EP_LOB_GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1550197 entries, 0 to 1550196\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count    Dtype         \n",
      "---  ------        --------------    -----         \n",
      " 0   EventId       1550197 non-null  int64         \n",
      " 1   PeriodId      1550197 non-null  int64         \n",
      " 2   EventDate     1550197 non-null  datetime64[ns]\n",
      " 3   Sum_Loss_sum  1550197 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
      "memory usage: 47.3 MB\n"
     ]
    }
   ],
   "source": [
    "dataframe_1_oep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate \n",
    "import decimal\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_and_save_parquet_2(dataframe_1_oep , parquet_file_path, speriod, samples):\n",
    "    dataframe_2 = dataframe_1_oep.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "    dataframe_3 = dataframe_1_oep.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * \n",
    "                              (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum()\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * \n",
    "                              (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        closest_index_3 = (dataframe_3['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_3]]])\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "       \n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_path_1=os.path.join(main_folder_path, 'EP', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Portfolio_GU_0.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    process_and_save_parquet_2(dataframe_1_oep, pq_file_path_1, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1550197 entries, 0 to 1550196\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count    Dtype         \n",
      "---  ------        --------------    -----         \n",
      " 0   EventId       1550197 non-null  int64         \n",
      " 1   PeriodId      1550197 non-null  int64         \n",
      " 2   EventDate     1550197 non-null  datetime64[ns]\n",
      " 3   Sum_Loss_sum  1550197 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
      "memory usage: 47.3 MB\n"
     ]
    }
   ],
   "source": [
    "dataframe_1_oep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>LobName</th>\n",
       "      <th>Sum_Loss_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53895970</td>\n",
       "      <td>207557</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.250859e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53891598</td>\n",
       "      <td>149148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.197889e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53893301</td>\n",
       "      <td>4312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.160561e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53893301</td>\n",
       "      <td>154312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.092983e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53891598</td>\n",
       "      <td>199148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.993543e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53893301</td>\n",
       "      <td>104312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.890958e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53874490</td>\n",
       "      <td>193067</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.782878e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53889489</td>\n",
       "      <td>239080</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.779150e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53889489</td>\n",
       "      <td>89080</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.744444e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53895970</td>\n",
       "      <td>107557</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.734298e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    EventId  PeriodId LobName  Sum_Loss_sum\n",
       "0  53895970    207557    SPER  2.250859e+10\n",
       "1  53891598    149148    SPER  2.197889e+10\n",
       "2  53893301      4312    SPER  2.160561e+10\n",
       "3  53893301    154312    SPER  2.092983e+10\n",
       "4  53891598    199148    SPER  1.993543e+10\n",
       "5  53893301    104312    SPER  1.890958e+10\n",
       "6  53874490    193067    SPER  1.782878e+10\n",
       "7  53889489    239080    SPER  1.779150e+10\n",
       "8  53889489     89080    SPER  1.744444e+10\n",
       "9  53895970    107557    SPER  1.734298e+10"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_1_oep.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_1_oep.to_csv(os.path.join(output_folder_path,'LOB_AUTO.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1550197 entries, 0 to 1550196\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count    Dtype         \n",
      "---  ------        --------------    -----         \n",
      " 0   LobName       1550197 non-null  object        \n",
      " 1   LobId         1550197 non-null  int64         \n",
      " 2   EventId       1550197 non-null  int64         \n",
      " 3   PeriodId      1550197 non-null  int64         \n",
      " 4   Weight        1550197 non-null  float64       \n",
      " 5   EventDate     1550197 non-null  datetime64[ns]\n",
      " 6   LossDate      1550197 non-null  datetime64[ns]\n",
      " 7   Region        1550197 non-null  object        \n",
      " 8   Peril         1550197 non-null  object        \n",
      " 9   Sum_Loss_sum  1550197 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(2), int64(3), object(3)\n",
      "memory usage: 118.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataframe_1_oep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LobName</th>\n",
       "      <th>LobId</th>\n",
       "      <th>EventId</th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>Weight</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>LossDate</th>\n",
       "      <th>Region</th>\n",
       "      <th>Peril</th>\n",
       "      <th>Sum_Loss_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53891598</td>\n",
       "      <td>149148</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.654085e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53891598</td>\n",
       "      <td>199148</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.509087e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53895970</td>\n",
       "      <td>207557</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.359457e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53893301</td>\n",
       "      <td>4312</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.319517e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53893301</td>\n",
       "      <td>154312</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.253157e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53893301</td>\n",
       "      <td>104312</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.200635e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53874490</td>\n",
       "      <td>193067</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-11-24</td>\n",
       "      <td>2020-11-24</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.184344e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53891598</td>\n",
       "      <td>49148</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.151871e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53891598</td>\n",
       "      <td>99148</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.135624e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUTO</td>\n",
       "      <td>2</td>\n",
       "      <td>53889489</td>\n",
       "      <td>239080</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>1.104149e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LobName  LobId   EventId  PeriodId    Weight  EventDate   LossDate Region  \\\n",
       "0    AUTO      2  53891598    149148  0.000004 2020-05-13 2020-05-13     EU   \n",
       "1    AUTO      2  53891598    199148  0.000004 2020-05-13 2020-05-13     EU   \n",
       "2    AUTO      2  53895970    207557  0.000004 2020-05-30 2020-05-30     EU   \n",
       "3    AUTO      2  53893301      4312  0.000004 2020-11-10 2020-11-10     EU   \n",
       "4    AUTO      2  53893301    154312  0.000004 2020-11-10 2020-11-10     EU   \n",
       "5    AUTO      2  53893301    104312  0.000004 2020-11-10 2020-11-10     EU   \n",
       "6    AUTO      2  53874490    193067  0.000004 2020-11-24 2020-11-24     EU   \n",
       "7    AUTO      2  53891598     49148  0.000004 2020-05-13 2020-05-13     EU   \n",
       "8    AUTO      2  53891598     99148  0.000004 2020-05-13 2020-05-13     EU   \n",
       "9    AUTO      2  53889489    239080  0.000004 2020-01-04 2020-01-04     EU   \n",
       "\n",
       "  Peril  Sum_Loss_sum  \n",
       "0    WS  1.654085e+09  \n",
       "1    WS  1.509087e+09  \n",
       "2    WS  1.359457e+09  \n",
       "3    WS  1.319517e+09  \n",
       "4    WS  1.253157e+09  \n",
       "5    WS  1.200635e+09  \n",
       "6    WS  1.184344e+09  \n",
       "7    WS  1.151871e+09  \n",
       "8    WS  1.135624e+09  \n",
       "9    WS  1.104149e+09  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_1_oep.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "No match for FieldRef.Name(EventDate) in EventId: int64 not null\nPeriodId: int64 not null\nSum_Loss: double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_Folder_TEST.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Perform final grouping and sorting\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m f_grouped_table_1 \u001b[39m=\u001b[39m final_table_1\u001b[39m.\u001b[39mgroup_by([\u001b[39m'\u001b[39m\u001b[39mEventId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPeriodId\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mEventDate\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39maggregate([(\u001b[39m'\u001b[39m\u001b[39mSum_Loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m f_grouped_table_2 \u001b[39m=\u001b[39m final_table_2\u001b[39m.\u001b[39;49mgroup_by([\u001b[39m'\u001b[39;49m\u001b[39mEventId\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mPeriodId\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mEventDate\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49maggregate([(\u001b[39m'\u001b[39;49m\u001b[39mSum_Loss\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m sorted_final_table_1 \u001b[39m=\u001b[39m f_grouped_table_1\u001b[39m.\u001b[39msort_by([(\u001b[39m'\u001b[39m\u001b[39mSum_Loss_sum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdescending\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m sorted_final_table_2 \u001b[39m=\u001b[39m f_grouped_table_2\u001b[39m.\u001b[39msort_by([(\u001b[39m'\u001b[39m\u001b[39mSum_Loss_sum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdescending\u001b[39m\u001b[39m'\u001b[39m)])\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\table.pxi:6509\u001b[0m, in \u001b[0;36mpyarrow.lib.TableGroupBy.aggregate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\acero.py:403\u001b[0m, in \u001b[0;36m_group_by\u001b[1;34m(table, aggregates, keys, use_threads)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_group_by\u001b[39m(table, aggregates, keys, use_threads\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    399\u001b[0m     decl \u001b[39m=\u001b[39m Declaration\u001b[39m.\u001b[39mfrom_sequence([\n\u001b[0;32m    400\u001b[0m         Declaration(\u001b[39m\"\u001b[39m\u001b[39mtable_source\u001b[39m\u001b[39m\"\u001b[39m, TableSourceNodeOptions(table)),\n\u001b[0;32m    401\u001b[0m         Declaration(\u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m, AggregateNodeOptions(aggregates, keys\u001b[39m=\u001b[39mkeys))\n\u001b[0;32m    402\u001b[0m     ])\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m decl\u001b[39m.\u001b[39;49mto_table(use_threads\u001b[39m=\u001b[39;49muse_threads)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\_acero.pyx:590\u001b[0m, in \u001b[0;36mpyarrow._acero.Declaration.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: No match for FieldRef.Name(EventDate) in EventId: int64 not null\nPeriodId: int64 not null\nSum_Loss: double"
     ]
    }
   ],
   "source": [
    "processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_table_1 = []\n",
    "final_grouped_table_2 = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, LobName, and LocationId\n",
    "    table = table.filter(pc.equal(table['LobName'], 'AUTO'))\n",
    "\n",
    "    grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    #grouped_table_1 = grouped_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "   # grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'Max_Loss'])\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table_2 = table.group_by(['EventId', 'PeriodId']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_2 = grouped_table_2.rename_columns(['EventId', 'PeriodId','Sum_Loss'])\n",
    "    \n",
    "    # Write intermediate results to disk\n",
    "    pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "    pq.write_table(grouped_table_2, os.path.join(processing_folder_path, f'grouped_table_2_{os.path.basename(file)}'))\n",
    "\n",
    "# Read all intermediate files and concatenate them\n",
    "intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "intermediate_files_2 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_2_')]\n",
    "\n",
    "final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "final_grouped_table_2 = [pq.read_table(f) for f in intermediate_files_2]\n",
    "\n",
    "final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "final_table_2 = pa.concat_tables(final_grouped_table_2)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId']).aggregate([('Sum_Loss', 'sum')])\n",
    "f_grouped_table_2 = final_table_2.group_by(['EventId', 'PeriodId']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "sorted_final_table_2 = f_grouped_table_2.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "dataframe_1_aep = sorted_final_table_2.to_pandas()\n",
    "\n",
    "# you can delete the steps below later\n",
    "\n",
    "# Write the final concatenated files to disk\n",
    "final_oep_path = os.path.join(output_folder_path, 'final_dataframe_1_oep.parquet')\n",
    "final_aep_path = os.path.join(output_folder_path, 'final_dataframe_1_aep.parquet')\n",
    "pq.write_table(sorted_final_table_1, final_oep_path)\n",
    "pq.write_table(sorted_final_table_2, final_aep_path)\n",
    "\n",
    "# Delete all non-concatenated files\n",
    "for f in intermediate_files_1 + intermediate_files_2:\n",
    "    os.remove(f)\n",
    "\n",
    "print(f'Final OEP file path: {final_oep_path}')\n",
    "print(f'Final AEP file path: {final_aep_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "flush_cache()\n",
    "speriod=50000\n",
    "samples=5\n",
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "output_folder_path = \"D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\"\n",
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OEP file path: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\final_dataframe_1_oep.parquet\n",
      "Final AEP file path: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\final_dataframe_1_aep.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_table_1 = []\n",
    "final_grouped_table_2 = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, LobName, and LocationId\n",
    "    grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'LobName', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'EventDate', 'Sum_Loss'])\n",
    "    #grouped_table_1 = grouped_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "   # grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'Max_Loss'])\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table_2 = table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_2 = grouped_table_2.rename_columns(['EventId', 'PeriodId', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "    # Write intermediate results to disk\n",
    "    pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "    pq.write_table(grouped_table_2, os.path.join(processing_folder_path, f'grouped_table_2_{os.path.basename(file)}'))\n",
    "\n",
    "# Read all intermediate files and concatenate them\n",
    "intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "intermediate_files_2 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_2_')]\n",
    "\n",
    "final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "final_grouped_table_2 = [pq.read_table(f) for f in intermediate_files_2]\n",
    "\n",
    "final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "final_table_2 = pa.concat_tables(final_grouped_table_2)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "f_grouped_table_2 = final_table_2.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "sorted_final_table_2 = f_grouped_table_2.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "dataframe_1_aep = sorted_final_table_2.to_pandas()\n",
    "\n",
    "# you can delete the steps below later\n",
    "\n",
    "# Write the final concatenated files to disk\n",
    "final_oep_path = os.path.join(output_folder_path, 'final_dataframe_1_oep.parquet')\n",
    "final_aep_path = os.path.join(output_folder_path, 'final_dataframe_1_aep.parquet')\n",
    "pq.write_table(sorted_final_table_1, final_oep_path)\n",
    "pq.write_table(sorted_final_table_2, final_aep_path)\n",
    "\n",
    "# Delete all non-concatenated files\n",
    "for f in intermediate_files_1 + intermediate_files_2:\n",
    "    os.remove(f)\n",
    "\n",
    "print(f'Final OEP file path: {final_oep_path}')\n",
    "print(f'Final AEP file path: {final_aep_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_2 = dataframe_1_oep.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "dataframe_2.to_csv(r'D:\\RISHIN\\Rough\\dataframe_grouped2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 248280 entries, 206130 to 42881\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   PeriodId  248280 non-null  int64  \n",
      " 1   Max_Loss  248280 non-null  float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "dataframe_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_4\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet\n"
     ]
    }
   ],
   "source": [
    "# Filter and assign to variables if not empty\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR'].empty:\n",
    "    daf_AGR_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO'].empty:\n",
    "    daf_AUTO_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM'].empty:\n",
    "    daf_COM_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND'].empty:\n",
    "    daf_IND_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER'].empty:\n",
    "    daf_SPER_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST'].empty:\n",
    "    daf_FRST_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH'].empty:\n",
    "    daf_GLH_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR'].empty:\n",
    "    daf_AGR_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO'].empty:\n",
    "    daf_AUTO_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM'].empty:\n",
    "    daf_COM_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND'].empty:\n",
    "    daf_IND_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER'].empty:\n",
    "    daf_SPER_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST'].empty:\n",
    "    daf_FRST_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH'].empty:\n",
    "    daf_GLH_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH']\n",
    "\n",
    "\n",
    "import decimal\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_and_save_parquet_2(dataframe_1_oep, dataframe_1_aep, parquet_file_path, speriod, samples):\n",
    "    # Group and aggregate dataframes\n",
    "    dataframe_2 = dataframe_1_oep.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "    dataframe_3 = dataframe_1_aep.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "    # Convert columns to decimal\n",
    "    dataframe_2['Max_Loss'] = dataframe_2['Max_Loss'].apply(lambda x: decimal.Decimal(str(x)))\n",
    "    dataframe_3['S_Sum_Loss'] = dataframe_3['S_Sum_Loss'].apply(lambda x: decimal.Decimal(str(x)))\n",
    "\n",
    "    # Calculate rates and cumulative rates using decimal.Decimal\n",
    "    rate = decimal.Decimal(1) / (decimal.Decimal(speriod) * decimal.Decimal(samples))\n",
    "    dataframe_2['rate'] = rate.quantize(decimal.Decimal('1.000000'))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().apply(lambda x: decimal.Decimal(str(x)).quantize(decimal.Decimal('1.000000')))\n",
    "    dataframe_2['RPs'] = dataframe_2['cumrate'].apply(lambda x: decimal.Decimal(1) / x)\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1).fillna(0)) * \n",
    "                              (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1).fillna(0)) * decimal.Decimal(0.5))\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().fillna(0).cumsum().apply(lambda x: decimal.Decimal(str(x))) * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3['rate'] = rate\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().apply(lambda x: decimal.Decimal(str(x)))\n",
    "    dataframe_3['RPs'] = dataframe_3['cumrate'].apply(lambda x: decimal.Decimal(1) / x)\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1).fillna(0)) * \n",
    "                              (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1).fillna(0)) * decimal.Decimal(0.5))\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().fillna(0).cumsum().apply(lambda x: decimal.Decimal(str(x))) * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    # Filter by RPs values\n",
    "    rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - decimal.Decimal(value)).abs().idxmin()\n",
    "        closest_index_3 = (dataframe_3['RPs'] - decimal.Decimal(value)).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_3]]])\n",
    "\n",
    "    # Rename columns\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    fdataframe_2['LobId'] = fdataframe_2['LobName'].map(lobname_to_lobid)\n",
    "    fdataframe_3['LobId'] = fdataframe_3['LobName'].map(lobname_to_lobid)\n",
    "\n",
    "    # Cast LobId to Decimal with precision 38 and scale 0\n",
    "    fdataframe_2['LobId'] = fdataframe_2['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "    fdataframe_3['LobId'] = fdataframe_3['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "\n",
    "    # Melt dataframes\n",
    "    columns_to_keep_3 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "    columns_to_keep_2 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "    # Concatenate final dataframes\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType','Loss'], ascending=[True, False])#.reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.decimal128(38, 18), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.decimal128(38, 18), nullable=True),\n",
    "        pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LobName', pa.string(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "# process_and_save_parquet_2(dataframe_1_oep, dataframe_1_aep, 'output.parquet', 100, 1000)\n",
    "    \n",
    "pq_file_path_1=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_0.parquet')\n",
    "\n",
    "pq_file_path_2=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_1.parquet')\n",
    "\n",
    "pq_file_path_3=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_2.parquet')\n",
    "\n",
    "pq_file_path_4=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_3.parquet')\n",
    "\n",
    "pq_file_path_5=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_4.parquet')\n",
    "\n",
    "pq_file_path_6=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_5.parquet')\n",
    "\n",
    "pq_file_path_7=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_AGR_oep, daf_AGR_aep, pq_file_path_1, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_AUTO_oep, daf_AUTO_aep, pq_file_path_2, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_COM_oep, daf_COM_aep, pq_file_path_3, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_IND_oep, daf_IND_aep, pq_file_path_4, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_SPER_oep, daf_SPER_aep, pq_file_path_5, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_FRST_oep, daf_FRST_aep, pq_file_path_6, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_GLH_oep, daf_GLH_aep, pq_file_path_7, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 8388608 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_Folder_TEST.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m intermediate_files_1 \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(processing_folder_path, f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(processing_folder_path) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mgrouped_table_1_\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m intermediate_files_2 \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(processing_folder_path, f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(processing_folder_path) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mgrouped_table_2_\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m final_grouped_table_1 \u001b[39m=\u001b[39m [pq\u001b[39m.\u001b[39;49mread_table(f) \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m intermediate_files_1]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m final_grouped_table_2 \u001b[39m=\u001b[39m [pq\u001b[39m.\u001b[39mread_table(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m intermediate_files_2]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m final_table_1 \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mconcat_tables(final_grouped_table_1)\n",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_Folder_TEST.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m intermediate_files_1 \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(processing_folder_path, f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(processing_folder_path) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mgrouped_table_1_\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m intermediate_files_2 \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(processing_folder_path, f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(processing_folder_path) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mgrouped_table_2_\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m final_grouped_table_1 \u001b[39m=\u001b[39m [pq\u001b[39m.\u001b[39;49mread_table(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m intermediate_files_1]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m final_grouped_table_2 \u001b[39m=\u001b[39m [pq\u001b[39m.\u001b[39mread_table(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m intermediate_files_2]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X54sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m final_table_1 \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mconcat_tables(final_grouped_table_1)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[39m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1832\u001b[0m     dataset \u001b[39m=\u001b[39m ParquetFile(\n\u001b[0;32m   1833\u001b[0m         source, read_dictionary\u001b[39m=\u001b[39mread_dictionary,\n\u001b[0;32m   1834\u001b[0m         memory_map\u001b[39m=\u001b[39mmemory_map, buffer_size\u001b[39m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         page_checksum_verification\u001b[39m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1841\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39;49mread(columns\u001b[39m=\u001b[39;49mcolumns, use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[0;32m   1844\u001b[0m                     use_pandas_metadata\u001b[39m=\u001b[39;49muse_pandas_metadata)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         index_columns \u001b[39m=\u001b[39m [\n\u001b[0;32m   1478\u001b[0m             col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1479\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mdict\u001b[39m)\n\u001b[0;32m   1480\u001b[0m         ]\n\u001b[0;32m   1481\u001b[0m         columns \u001b[39m=\u001b[39m (\n\u001b[0;32m   1482\u001b[0m             \u001b[39mlist\u001b[39m(columns) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(index_columns) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(columns))\n\u001b[0;32m   1483\u001b[0m         )\n\u001b[1;32m-> 1485\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset\u001b[39m.\u001b[39;49mto_table(\n\u001b[0;32m   1486\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_expression,\n\u001b[0;32m   1487\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads\n\u001b[0;32m   1488\u001b[0m )\n\u001b[0;32m   1490\u001b[0m \u001b[39m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[39m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[39mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\_dataset.pyx:562\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3841\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 8388608 failed"
     ]
    }
   ],
   "source": [
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import os\n",
    "\n",
    "# processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "# os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# # Initialize an empty list to store the results\n",
    "# final_grouped_table_1 = []\n",
    "# final_grouped_table_2 = []\n",
    "\n",
    "# # Process each Parquet file individually\n",
    "# for file in parquet_files:\n",
    "#     # Read the Parquet file into a PyArrow Table\n",
    "#     table = pq.read_table(file)\n",
    "    \n",
    "#     # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, LobName, and LocationId\n",
    "#     grouped_table_1 = table.group_by(['EventId', 'PeriodId','Loss' , 'LobName', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "#     grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId','Loss', 'LobName', 'EventDate', 'Sum_Loss'])\n",
    "#     #grouped_table_1 = grouped_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "#     #grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'Max_Loss'])\n",
    "    \n",
    "#     # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "#     grouped_table_2= table.group_by(['EventId', 'PeriodId','Loss','LobName']).aggregate([('Loss', 'sum')])\n",
    "#     grouped_table_2 = grouped_table_2.rename_columns(['EventId', 'PeriodId','Loss', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "#     # Write intermediate results to disk\n",
    "#     pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "#     pq.write_table(grouped_table_2, os.path.join(processing_folder_path, f'grouped_table_2_{os.path.basename(file)}'))\n",
    "\n",
    "# # Read all intermediate files and concatenate them\n",
    "# intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "# intermediate_files_2 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_2_')]\n",
    "\n",
    "# final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "# final_grouped_table_2 = [pq.read_table(f) for f in intermediate_files_2]\n",
    "\n",
    "# final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "# final_table_2 = pa.concat_tables(final_grouped_table_2)\n",
    "\n",
    "# # Perform final grouping and sorting\n",
    "# f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'LobName','Loss']).aggregate([('Sum_Loss', 'sum')])\n",
    "# f_grouped_table_2 = final_table_2.group_by(['EventId', 'PeriodId', 'LobName','Loss']).aggregate([('Sum_Loss', 'sum')])\n",
    "# sorted_final_table_1 = f_grouped_table_1.sort_by([('Loss', 'descending')])\n",
    "# sorted_final_table_2 = f_grouped_table_2.sort_by([('Loss', 'descending')])\n",
    "\n",
    "# # Convert to pandas DataFrames\n",
    "# dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "# dataframe_1_aep = sorted_final_table_2.to_pandas()\n",
    "\n",
    "# # you can delete the steps below later\n",
    "\n",
    "# # Write the final concatenated files to disk\n",
    "# final_oep_path = os.path.join(output_folder_path, 'final_dataframe_1_oep.parquet')\n",
    "# final_aep_path = os.path.join(output_folder_path, 'final_dataframe_1_aep.parquet')\n",
    "# pq.write_table(sorted_final_table_1, final_oep_path)\n",
    "# pq.write_table(sorted_final_table_2, final_aep_path)\n",
    "\n",
    "# # Delete all non-concatenated files\n",
    "# for f in intermediate_files_1 + intermediate_files_2:\n",
    "#     os.remove(f)\n",
    "\n",
    "# print(f'Final OEP file path: {final_oep_path}')\n",
    "# print(f'Final AEP file path: {final_aep_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_Folder_TEST.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     daf_IND_oep \u001b[39m=\u001b[39m dataframe_1_oep[dataframe_1_oep[\u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mIND\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X53sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dataframe_1_oep[dataframe_1_oep[\u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSPER\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mempty:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X53sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     daf_SPER_oep \u001b[39m=\u001b[39m dataframe_1_oep[dataframe_1_oep[\u001b[39m'\u001b[39;49m\u001b[39mLobName\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mSPER\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X53sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dataframe_1_oep[dataframe_1_oep[\u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFRST\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mempty:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_Folder_TEST.ipynb#X53sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     daf_FRST_oep \u001b[39m=\u001b[39m dataframe_1_oep[dataframe_1_oep[\u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFRST\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6121\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    343\u001b[0m \u001b[39melif\u001b[39;00m lvalues\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 344\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:130\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mscalar_compare(x\u001b[39m.\u001b[39mravel(), y, op)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39;49mreshape(x\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Filter and assign to variables if not empty\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR'].empty:\n",
    "#     daf_AGR_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO'].empty:\n",
    "#     daf_AUTO_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM'].empty:\n",
    "#     daf_COM_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND'].empty:\n",
    "#     daf_IND_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER'].empty:\n",
    "#     daf_SPER_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST'].empty:\n",
    "#     daf_FRST_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH'].empty:\n",
    "#     daf_GLH_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR'].empty:\n",
    "#     daf_AGR_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO'].empty:\n",
    "#     daf_AUTO_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM'].empty:\n",
    "#     daf_COM_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND'].empty:\n",
    "#     daf_IND_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER'].empty:\n",
    "#     daf_SPER_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST'].empty:\n",
    "#     daf_FRST_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH'].empty:\n",
    "#     daf_GLH_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH']\n",
    "\n",
    "\n",
    "# import decimal\n",
    "# import pandas as pd\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# def process_and_save_parquet_2(dataframe_1_oep, dataframe_1_aep, parquet_file_path, speriod, samples):\n",
    "#     # Group and aggregate dataframes\n",
    "#     dataframe_2 = dataframe_1_oep.groupby(['PeriodId', 'LobName','Loss'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "#     dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "#     dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "#     dataframe_3 = dataframe_1_aep.groupby(['PeriodId', 'LobName','Loss'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "#     dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "#     dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "#     # Convert columns to decimal\n",
    "#     dataframe_2['Max_Loss'] = dataframe_2['Max_Loss'].apply(lambda x: decimal.Decimal(str(x)))\n",
    "#     dataframe_3['S_Sum_Loss'] = dataframe_3['S_Sum_Loss'].apply(lambda x: decimal.Decimal(str(x)))\n",
    "\n",
    "#     # Calculate rates and cumulative rates using decimal.Decimal\n",
    "#     rate = decimal.Decimal(1) / (decimal.Decimal(speriod) * decimal.Decimal(samples))\n",
    "#     dataframe_2['rate'] = rate.quantize(decimal.Decimal('1.000000'))\n",
    "#     dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().apply(lambda x: decimal.Decimal(str(x)).quantize(decimal.Decimal('1.000000')))\n",
    "#     dataframe_2['RPs'] = dataframe_2['cumrate'].apply(lambda x: decimal.Decimal(1) / x)\n",
    "#     dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1).fillna(0)) * \n",
    "#                               (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1).fillna(0)) * decimal.Decimal(0.5))\n",
    "#     dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().fillna(0).cumsum().apply(lambda x: decimal.Decimal(str(x))) * dataframe_2['RPs'])\n",
    "#     dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "#     dataframe_3['rate'] = rate\n",
    "#     dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().apply(lambda x: decimal.Decimal(str(x)))\n",
    "#     dataframe_3['RPs'] = dataframe_3['cumrate'].apply(lambda x: decimal.Decimal(1) / x)\n",
    "#     dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1).fillna(0)) * \n",
    "#                               (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1).fillna(0)) * decimal.Decimal(0.5))\n",
    "#     dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().fillna(0).cumsum().apply(lambda x: decimal.Decimal(str(x))) * dataframe_3['RPs'])\n",
    "#     dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "#     # Filter by RPs values\n",
    "#     rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "#     fdataframe_2 = pd.DataFrame()\n",
    "#     fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "#     for value in rps_values:\n",
    "#         closest_index_2 = (dataframe_2['RPs'] - decimal.Decimal(value)).abs().idxmin()\n",
    "#         closest_index_3 = (dataframe_3['RPs'] - decimal.Decimal(value)).abs().idxmin()\n",
    "#         fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "#         fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_3]]])\n",
    "\n",
    "#     # Rename columns\n",
    "#     fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "#     fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "#     # Map LobName to LobId\n",
    "#     lobname_to_lobid = {\n",
    "#         'AGR': 1,\n",
    "#         'AUTO': 2,\n",
    "#         'COM': 3,\n",
    "#         'IND': 4,\n",
    "#         'SPER': 5,\n",
    "#         'FRST': 6,\n",
    "#         'GLH': 7\n",
    "#     }\n",
    "\n",
    "#     fdataframe_2['LobId'] = fdataframe_2['LobName'].map(lobname_to_lobid)\n",
    "#     fdataframe_3['LobId'] = fdataframe_3['LobName'].map(lobname_to_lobid)\n",
    "\n",
    "#     # Cast LobId to Decimal with precision 38 and scale 0\n",
    "#     fdataframe_2['LobId'] = fdataframe_2['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "#     fdataframe_3['LobId'] = fdataframe_3['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "\n",
    "#     # Melt dataframes\n",
    "#     columns_to_keep_3 = ['RPs', 'LobId', 'LobName']\n",
    "#     columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "#     melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "#                                     var_name='EPType', value_name='Loss_c')\n",
    "#     melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_3 = melted_df_3[['EPType', 'Loss','Loss_c','ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "#     columns_to_keep_2 = ['RPs', 'LobId', 'LobName']\n",
    "#     columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "#     melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "#                                     var_name='EPType', value_name='Loss')\n",
    "#     melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_2 = melted_df_2[['EPType', 'Loss', 'Loss_c','ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "#     # Concatenate final dataframes\n",
    "#     final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "#     new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "#     final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "#     final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType','Loss'], ascending=[True, False])#.reset_index(drop=True)\n",
    "\n",
    "#     # Define the schema to match the required Parquet file schema\n",
    "#     schema = pa.schema([\n",
    "#         pa.field('EPType', pa.string(), nullable=True),\n",
    "#         pa.field('Loss', pa.decimal128(38, 18), nullable=True),\n",
    "#         pa.field('Loss_c', pa.decimal128(38, 18), nullable=True),\n",
    "#         pa.field('ReturnPeriod', pa.decimal128(38, 18), nullable=True),\n",
    "#         pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "#         pa.field('LobName', pa.string(), nullable=True)\n",
    "#     ])\n",
    "\n",
    "#     # Convert DataFrame to Arrow Table with the specified schema\n",
    "#     table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "#     # Save to Parquet\n",
    "#     pq.write_table(table, parquet_file_path)\n",
    "\n",
    "#     print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# # Example usage\n",
    "# # process_and_save_parquet_2(dataframe_1_oep, dataframe_1_aep, 'output.parquet', 100, 1000)\n",
    "    \n",
    "# pq_file_path_1=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_0.parquet')\n",
    "\n",
    "# pq_file_path_2=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_1.parquet')\n",
    "\n",
    "# pq_file_path_3=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_2.parquet')\n",
    "\n",
    "# pq_file_path_4=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_3.parquet')\n",
    "\n",
    "# pq_file_path_5=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_4.parquet')\n",
    "\n",
    "# pq_file_path_6=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_5.parquet')\n",
    "\n",
    "# pq_file_path_7=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "\n",
    "# try:\n",
    "#     process_and_save_parquet_2(daf_AGR_oep, daf_AGR_aep, pq_file_path_1, speriod, samples)\n",
    "# except (NameError, AttributeError):\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     process_and_save_parquet_2(daf_AUTO_oep, daf_AUTO_aep, pq_file_path_2, speriod, samples)\n",
    "# except (NameError, AttributeError):\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     process_and_save_parquet_2(daf_COM_oep, daf_COM_aep, pq_file_path_3, speriod, samples)\n",
    "# except (NameError, AttributeError):\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     process_and_save_parquet_2(daf_IND_oep, daf_IND_aep, pq_file_path_4, speriod, samples)\n",
    "# except (NameError, AttributeError):\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     process_and_save_parquet_2(daf_SPER_oep, daf_SPER_aep, pq_file_path_5, speriod, samples)\n",
    "# except (NameError, AttributeError):\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     process_and_save_parquet_2(daf_FRST_oep, daf_FRST_aep, pq_file_path_6, speriod, samples)\n",
    "# except (NameError, AttributeError):\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     process_and_save_parquet_2(daf_GLH_oep, daf_GLH_aep, pq_file_path_7, speriod, samples)\n",
    "# except (NameError, AttributeError):\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
