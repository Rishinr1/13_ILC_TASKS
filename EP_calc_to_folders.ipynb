{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EP LOB GU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_tables = []\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table = table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Rename the aggregated column to Sum_Loss\n",
    "    grouped_table = grouped_table.rename_columns(['EventId', 'PeriodId', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "    # Append the grouped Table to the final_grouped_tables list\n",
    "    final_grouped_tables.append(grouped_table)\n",
    "\n",
    "# Concatenate all grouped tables\n",
    "final_table = pa.concat_tables(final_grouped_tables)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "final_grouped_table = final_table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table = final_grouped_table.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "# The Table is now ready for the next instructions\n",
    "dataframe_1 = sorted_final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_1= dataframe_1.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataframe_1[dataframe_1['LobName'] == 'AGR'].empty:\n",
    "    daf_AGR = dataframe_1[dataframe_1['LobName'] == 'AGR']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'AUTO'].empty:\n",
    "    daf_AUTO = dataframe_1[dataframe_1['LobName'] == 'AUTO']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'COM'].empty:\n",
    "    daf_COM = dataframe_1[dataframe_1['LobName'] == 'COM']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'IND'].empty:\n",
    "    daf_IND = dataframe_1[dataframe_1['LobName'] == 'IND']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'SPER'].empty:\n",
    "    daf_SPER = dataframe_1[dataframe_1['LobName'] == 'SPER']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'FRST'].empty:\n",
    "    daf_FRST = dataframe_1[dataframe_1['LobName'] == 'FRST']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'GLH'].empty:\n",
    "    daf_GLH = dataframe_1[dataframe_1['LobName'] == 'GLH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_save_parquet(dataframe_1, parquet_file_path, speriod, samples):\n",
    "    # Initialize dataframe_2 by selecting PeriodId and max(Sum_Loss) grouped by PeriodId\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "\n",
    "    # Rename the aggregated column to Max_Loss\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "\n",
    "    # Sort dataframe_2 by Max_Loss in descending order\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "    # Initialize dataframe_3 by selecting PeriodId and Sum(Sum_Loss) grouped by PeriodId\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "\n",
    "    # Rename the aggregated column to Sum_Loss\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "\n",
    "    # Sort dataframe_3 by S_sum_Loss in descending order\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "\n",
    "    # Calculate the cumulative rate column and round to 6 decimal places\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum()\n",
    "\n",
    "    # Calculate the RPs column and round to 6 decimal places\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "    # Calculate the TCE_OEP_1 column and round to 6 decimal places\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * \n",
    "                              (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "\n",
    "    # Calculate the TCE_OEP_2 column and round to 6 decimal places\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "\n",
    "    # Calculate the TCE_OEP_Final column and round to 6 decimal places\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "\n",
    "    # Calculate the cumulative rate column and round to 6 decimal places\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum()\n",
    "\n",
    "    # Calculate the RPs column and round to 6 decimal places\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "    # Calculate the TCE_AEP_1 column and round to 6 decimal places\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * \n",
    "                              (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "\n",
    "    # Calculate the cumulative sum up to the previous row and multiply by the current row's RPs, then round to 6 decimal places\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "\n",
    "    # Calculate the TCE_AEP_Final column and round to 6 decimal places\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    # Define the list of RPs values to filter and convert them to float\n",
    "    rps_values = [float(x) for x in [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]]\n",
    "\n",
    "    # Initialize an empty DataFrame to store the filtered results\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    # Define the number of decimal places to round to\n",
    "    decimal_places = 8\n",
    "\n",
    "    # Loop through each value in rps_values and filter the DataFrames\n",
    "    for value in rps_values:\n",
    "        rounded_value = round(value, decimal_places)\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2[np.round(dataframe_2['RPs'], decimal_places) == rounded_value]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3[np.round(dataframe_3['RPs'], decimal_places) == rounded_value]])\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': \"1\",\n",
    "        'AUTO': \"2\",\n",
    "        'COM': \"3\",\n",
    "        'IND': \"4\",\n",
    "        'SPER': \"5\",\n",
    "        'FRST': \"6\",\n",
    "        'GLH': \"7\"\n",
    "    }\n",
    "\n",
    "    # Add the LobId column to fdataframe_2\n",
    "    fdataframe_2['LobId'] = fdataframe_2['LobName'].map(lobname_to_lobid)\n",
    "\n",
    "    # Add the LobId column to fdataframe_3\n",
    "    fdataframe_3['LobId'] = fdataframe_3['LobName'].map(lobname_to_lobid)\n",
    "\n",
    "    # Define the columns to be used in the new DataFrame for fdataframe_3\n",
    "    columns_to_keep_3 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "\n",
    "    # Melt fdataframe_3 to reshape it\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "\n",
    "    # Rename columns to match the desired output\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "    # Define the columns to be used in the new DataFrame for fdataframe_2\n",
    "    columns_to_keep_2 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "\n",
    "    # Melt fdataframe_2 to reshape it\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "\n",
    "    # Rename columns to match the desired output\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "\n",
    "    # Define the new order for EPType\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "\n",
    "    # Update the EPType column to the new order\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "\n",
    "    # Sort the DataFrame by EPType and then by ReturnPeriod in descending order within each EPType\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Save final_df as a Parquet file\n",
    "    final_df_EP_LOB_GU.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_path_1=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_0.parquet')\n",
    "\n",
    "pq_file_path_2=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_1.parquet')\n",
    "\n",
    "pq_file_path_3=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_2.parquet')\n",
    "\n",
    "pq_file_path_4=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_3.parquet')\n",
    "\n",
    "pq_file_path_5=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_4.parquet')\n",
    "\n",
    "pq_file_path_6=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_5.parquet')\n",
    "\n",
    "pq_file_path_7=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    process_and_save_parquet(daf_AGR, pq_file_path_1, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_AUTO, pq_file_path_2, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_COM, pq_file_path_3, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_IND, pq_file_path_4, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_SPER, pq_file_path_5, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_FRST, pq_file_path_6, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet(daf_GLH, pq_file_path_7, speriod, samples)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for EP lob portfoilio GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_tables = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table = table.group_by(['EventId', 'PeriodId']).aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Rename the aggregated column to Sum_Loss\n",
    "    grouped_table = grouped_table.rename_columns(['EventId', 'PeriodId', 'Sum_Loss'])\n",
    "    \n",
    "    # Append the grouped Table to the final_grouped_tables list\n",
    "    final_grouped_tables.append(grouped_table)\n",
    "\n",
    "# Concatenate all grouped tables\n",
    "final_table = pa.concat_tables(final_grouped_tables)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "final_grouped_table = final_table.group_by(['EventId', 'PeriodId']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table = final_grouped_table.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "# The Table is now ready for the next instructions\n",
    "dataframe_1 = sorted_final_table\n",
    "dataframe_1= dataframe_1.to_pandas()\n",
    "#dataframe_1 = dataframe_1[dataframe_1['LobName'] == 'AUTO']\n",
    "\n",
    "# Initialize dataframe_2 by selecting PeriodId and max(Sum_Loss) grouped by PeriodId\n",
    "dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "\n",
    "# Rename the aggregated column to Max_Loss\n",
    "dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "\n",
    "# Sort dataframe_2 by Max_Loss in descending order\n",
    "dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "# Initialize dataframe_2 by selecting PeriodId and Sum(Sum_Loss) grouped by PeriodId\n",
    "dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "\n",
    "# Rename the aggregated column to Sum_Loss\n",
    "dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "\n",
    "# Sort dataframe_3 by S_sum_Loss in descending order\n",
    "dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "#dataframe_2['Max_Loss'] = dataframe_2['Max_Loss'].round(5)\n",
    "\n",
    "dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "\n",
    "# Calculate the cumulative rate column and round to 6 decimal places\n",
    "dataframe_2['cumrate'] = dataframe_2['rate'].cumsum()\n",
    "\n",
    "# Calculate the RPs column and round to 6 decimal places\n",
    "dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the TCE_OEP_1 column and round to 6 decimal places\n",
    "dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * \n",
    "                          (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "\n",
    "# Calculate the TCE_OEP_2 column and round to 6 decimal places\n",
    "dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "\n",
    "\n",
    "# Calculate the TCE_OEP_Final column and round to 6 decimal places\n",
    "dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "#dataframe_3['S_Sum_Loss'] = dataframe_3['S_Sum_Loss'].round(5)\n",
    "\n",
    "# Calculate the rate column and round to 6 decimal places\n",
    "dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "\n",
    "# Calculate the cumulative rate column and round to 6 decimal places\n",
    "dataframe_3['cumrate'] = dataframe_3['rate'].cumsum()\n",
    "\n",
    "# Calculate the RPs column and round to 6 decimal places\n",
    "dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "# Calculate the TCE_AEP_1 column and round to 6 decimal places\n",
    "dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * \n",
    "                          (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "\n",
    "# Calculate the cumulative sum up to the previous row and multiply by the current row's RPs, then round to 6 decimal places\n",
    "dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "\n",
    "# Calculate the TCE_AEP_Final column and round to 6 decimal places\n",
    "dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "# Define the list of RPs values to filter and convert them to float\n",
    "rps_values = [float(x) for x in [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]]\n",
    "\n",
    "# Initialize an empty DataFrame to store the filtered results\n",
    "fdataframe_2 = pd.DataFrame()\n",
    "fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "# Define the number of decimal places to round to\n",
    "decimal_places = 8\n",
    "\n",
    "# Loop through each value in rps_values and filter the DataFrames\n",
    "for value in rps_values:\n",
    "    rounded_value = round(value, decimal_places)\n",
    "    fdataframe_2 = pd.concat([fdataframe_2, dataframe_2[np.round(dataframe_2['RPs'], decimal_places) == rounded_value]])\n",
    "    fdataframe_3 = pd.concat([fdataframe_3, dataframe_3[np.round(dataframe_3['RPs'], decimal_places) == rounded_value]])\n",
    "\n",
    "\n",
    "fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP','TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "fdataframe_2.rename(columns={ 'Max_Loss': 'OEP','TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "# Define the columns to be used in the new DataFrame for fdataframe_3\n",
    "columns_to_keep_3 = ['RPs']\n",
    "columns_to_melt_3 = [ 'AEP','TCE-AEP']\n",
    "\n",
    "# Melt fdataframe_3 to reshape it\n",
    "melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "                                var_name='EPType', value_name='Loss')\n",
    "\n",
    "# Rename columns to match the desired output\n",
    "melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "# Define the columns to be used in the new DataFrame for fdataframe_2\n",
    "columns_to_keep_2 = ['RPs']\n",
    "columns_to_melt_2 = [ 'OEP','TCE-OEP']\n",
    "\n",
    "# Melt fdataframe_2 to reshape it\n",
    "melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "                                var_name='EPType', value_name='Loss')\n",
    "\n",
    "# Rename columns to match the desired output\n",
    "melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "final_df_EP_Portfolio_GU = pd.concat([ final_df_2,final_df_3], ignore_index=True)\n",
    "\n",
    "\n",
    "# Define the new order for EPType\n",
    "new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "\n",
    "# Update the EPType column to the new order\n",
    "final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by EPType and then by ReturnPeriod in descending order within each EPType\n",
    "final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'EP', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Portfolio_GU_0.parquet')\n",
    "\n",
    "# Save final_df as a Parquet file\n",
    "final_df_EP_Portfolio_GU.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for stats LOB GU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the aggregated results\n",
    "aggregated_tables = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "    grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Calculate AAL\n",
    "    loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "    aal = loss_sum / speriod / samples\n",
    "    aal_array = pa.array(aal)\n",
    "    grouped = grouped.append_column('AAL', aal_array)\n",
    "    \n",
    "    # Select only the necessary columns\n",
    "    grouped = grouped.select(['LobName', 'AAL'])\n",
    "    \n",
    "    # Append the grouped Table to the list\n",
    "    aggregated_tables.append(grouped)\n",
    "\n",
    "# Concatenate all the grouped Tables\n",
    "final_table = pa.concat_tables(aggregated_tables)\n",
    "\n",
    "# Group the final Table again to ensure all groups are combined\n",
    "final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "# Sort the final grouped Table by 'LobName'\n",
    "final_grouped = final_grouped.sort_by('LobName')\n",
    "\n",
    "# Convert the final grouped Table to a Pandas DataFrame\n",
    "final_df = final_grouped.to_pandas()\n",
    "\n",
    "final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid)\n",
    "\n",
    "final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "# Define the columns with NaN values for 'Std' and 'CV'\n",
    "final_df_STATS_Lob['Std'] = np.nan\n",
    "final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "# Reorder the columns to match the specified format\n",
    "final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GU_0.parquet')\n",
    "final_df_STATS_Lob.to_parquet(parquet_file_path, index=False)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for STATS Portfolio GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aggregated_tables = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "    grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Calculate AAL\n",
    "    loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "    aal = loss_sum / speriod / samples\n",
    "    aal_array = pa.array(aal)\n",
    "    grouped = grouped.append_column('AAL', aal_array)\n",
    "    \n",
    "    # Select only the necessary columns\n",
    "    grouped = grouped.select(['LobName', 'AAL'])\n",
    "    \n",
    "    # Append the grouped Table to the list\n",
    "    aggregated_tables.append(grouped)\n",
    "\n",
    "# Concatenate all the grouped Tables\n",
    "final_table = pa.concat_tables(aggregated_tables)\n",
    "\n",
    "# Convert the final table to a Pandas DataFrame\n",
    "final_df = final_table.to_pandas()\n",
    "\n",
    "# Sum all the AAL values without grouping by LobName\n",
    "total_aal = final_df['AAL'].sum()\n",
    "\n",
    "# Create a DataFrame with the specified columns\n",
    "final_df_STATS_Portfolio = pd.DataFrame({\n",
    "    'AAL': [total_aal],\n",
    "    'Std': [np.nan],\n",
    "    'CV': [np.nan],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GU_0.parquet')\n",
    "final_df_STATS_Portfolio.to_parquet(parquet_file_path, index=False)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLT GU Lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    parquet_file = pq.ParquetFile(file)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    parquet_file = pq.ParquetFile(file)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "# Perform the final group by and aggregation\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "\n",
    "# Rename the aggregated column\n",
    "final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "\n",
    "# Convert the result to a Pandas DataFrame\n",
    "df_grouped = final_grouped_table.to_pandas()\n",
    "\n",
    "# Delete intermediate files\n",
    "for file in intermediate_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Remove the intermediate directory\n",
    "try:\n",
    "    os.rmdir(intermediate_dir)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {intermediate_dir}\")\n",
    "except OSError:\n",
    "    print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "# Reorder the columns in the desired order\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "df_grouped = df_grouped[ordered_columns]\n",
    "df_grouped.to_parquet(parquet_file_path, index=False)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"D:\\RISHIN\\13_ILC_TASK1\\input\\EXPORT\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GU\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter the list to include only Parquet files\n",
    "parquet_files = [f for f in files if f.endswith('.parquet')]\n",
    "\n",
    "# Define the schema with logical types for timestamps and decimals\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC')), nullable=True, metadata={'field_id': '-1'},\n",
    "    pa.field('Loss', pa.float64()),\n",
    "    pa.field('Region', pa.string()),\n",
    "    pa.field('Peril', pa.string()),\n",
    "    pa.field('Weight', pa.float64()),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('LobName', pa.string())\n",
    "])\n",
    "\n",
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(folder_path, 'intermediate_results')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch], schema=schema)\n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "# Perform the final group by and aggregation\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "# Rename the aggregated column\n",
    "final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "# Convert the result to a Pandas DataFrame\n",
    "df_grouped = final_grouped_table.to_pandas()\n",
    "\n",
    "# Delete intermediate files\n",
    "for file in intermediate_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Remove the intermediate directory\n",
    "try:\n",
    "    os.rmdir(intermediate_dir)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {intermediate_dir}\")\n",
    "except OSError:\n",
    "    print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "os.makedirs(main_folder_path, exist_ok=True)\n",
    "parquet_file_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "# Reorder the columns in the desired order\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "df_grouped = df_grouped[ordered_columns]\n",
    "\n",
    "# Save the final DataFrame to a Parquet file\n",
    "df_grouped.to_parquet(parquet_file_path, index=False)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLT Portfolio GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "country=\"BE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "# Flush memory at the beginning\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "gc.collect()\n",
    "\n",
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_G.parquet')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = [\"PeriodId\", \"EventId\", \"EventDate\", \"LossDate\", \"Region\", \"Peril\", \"Weight\"]\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    parquet_file = pq.ParquetFile(file)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "# Perform the final group by and aggregation\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "# Rename the aggregated column\n",
    "final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "# Convert PeriodId and EventId to strings\n",
    "final_grouped_table = final_grouped_table.set_column(\n",
    "    final_grouped_table.schema.get_field_index('PeriodId'),\n",
    "    'PeriodId',\n",
    "    final_grouped_table.column('PeriodId').cast(pa.string())\n",
    ")\n",
    "final_grouped_table = final_grouped_table.set_column(\n",
    "    final_grouped_table.schema.get_field_index('EventId'),\n",
    "    'EventId',\n",
    "    final_grouped_table.column('EventId').cast(pa.string())\n",
    ")\n",
    "\n",
    "# Define the schema\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Loss', pa.float64(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Region', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Peril', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Weight', pa.float64(), nullable=True, metadata={'field_id': '-1'})\n",
    "])\n",
    "\n",
    "# Convert the table to the specified schema\n",
    "final_grouped_table = pa.Table.from_arrays(\n",
    "    [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Write the table to a Parquet file with the specified schema\n",
    "parquet_file_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GU_0.parquet')\n",
    "final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "pq.write_table(final_grouped_table, parquet_file_path)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Delete intermediate files\n",
    "for file in intermediate_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Remove the intermediate directory\n",
    "try:\n",
    "    os.rmdir(intermediate_dir)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {intermediate_dir}\")\n",
    "except OSError:\n",
    "    print(f\"Directory not empty or other error: {intermediate_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Prompt the user to enter the path to the Parquet file\n",
    "file_path = input(\"Enter the file path: \")\n",
    "\n",
    "# Ensure the file path is treated as a raw string and strip any extra quotes\n",
    "file_path = r\"{}\".format(file_path.strip('\"'))\n",
    "\n",
    "# Read the Parquet file into a PyArrow Table\n",
    "table = pq.read_table(file_path)\n",
    "\n",
    "# Convert the PyArrow Table to a Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>LossDate</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Region</th>\n",
       "      <th>Peril</th>\n",
       "      <th>Weight</th>\n",
       "      <th>LobId</th>\n",
       "      <th>LobName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207557</td>\n",
       "      <td>53895970</td>\n",
       "      <td>2020-05-30 00:00:00+00:00</td>\n",
       "      <td>2020-05-30 00:00:00+00:00</td>\n",
       "      <td>2.250859e+10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>SPER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149148</td>\n",
       "      <td>53891598</td>\n",
       "      <td>2020-05-13 00:00:00+00:00</td>\n",
       "      <td>2020-05-13 00:00:00+00:00</td>\n",
       "      <td>2.197889e+10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>SPER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4312</td>\n",
       "      <td>53893301</td>\n",
       "      <td>2020-11-10 00:00:00+00:00</td>\n",
       "      <td>2020-11-10 00:00:00+00:00</td>\n",
       "      <td>2.160561e+10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>SPER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154312</td>\n",
       "      <td>53893301</td>\n",
       "      <td>2020-11-10 00:00:00+00:00</td>\n",
       "      <td>2020-11-10 00:00:00+00:00</td>\n",
       "      <td>2.092983e+10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>SPER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199148</td>\n",
       "      <td>53891598</td>\n",
       "      <td>2020-05-13 00:00:00+00:00</td>\n",
       "      <td>2020-05-13 00:00:00+00:00</td>\n",
       "      <td>1.993543e+10</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>SPER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381503</th>\n",
       "      <td>165828</td>\n",
       "      <td>53876702</td>\n",
       "      <td>2020-10-21 00:00:00+00:00</td>\n",
       "      <td>2020-10-21 00:00:00+00:00</td>\n",
       "      <td>1.870107e-03</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2</td>\n",
       "      <td>AUTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381504</th>\n",
       "      <td>130081</td>\n",
       "      <td>53865125</td>\n",
       "      <td>2020-12-14 00:00:00+00:00</td>\n",
       "      <td>2020-12-14 00:00:00+00:00</td>\n",
       "      <td>1.669423e-03</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>AGR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381505</th>\n",
       "      <td>82560</td>\n",
       "      <td>53881107</td>\n",
       "      <td>2020-10-25 00:00:00+00:00</td>\n",
       "      <td>2020-10-25 00:00:00+00:00</td>\n",
       "      <td>8.474226e-04</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2</td>\n",
       "      <td>AUTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381506</th>\n",
       "      <td>212031</td>\n",
       "      <td>53872002</td>\n",
       "      <td>2020-12-16 00:00:00+00:00</td>\n",
       "      <td>2020-12-16 00:00:00+00:00</td>\n",
       "      <td>6.278728e-04</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>AGR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381507</th>\n",
       "      <td>249127</td>\n",
       "      <td>53885692</td>\n",
       "      <td>2020-01-16 00:00:00+00:00</td>\n",
       "      <td>2020-01-16 00:00:00+00:00</td>\n",
       "      <td>3.690754e-04</td>\n",
       "      <td>EU</td>\n",
       "      <td>WS</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>5</td>\n",
       "      <td>SPER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7381508 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PeriodId   EventId                 EventDate  \\\n",
       "0         207557  53895970 2020-05-30 00:00:00+00:00   \n",
       "1         149148  53891598 2020-05-13 00:00:00+00:00   \n",
       "2           4312  53893301 2020-11-10 00:00:00+00:00   \n",
       "3         154312  53893301 2020-11-10 00:00:00+00:00   \n",
       "4         199148  53891598 2020-05-13 00:00:00+00:00   \n",
       "...          ...       ...                       ...   \n",
       "7381503   165828  53876702 2020-10-21 00:00:00+00:00   \n",
       "7381504   130081  53865125 2020-12-14 00:00:00+00:00   \n",
       "7381505    82560  53881107 2020-10-25 00:00:00+00:00   \n",
       "7381506   212031  53872002 2020-12-16 00:00:00+00:00   \n",
       "7381507   249127  53885692 2020-01-16 00:00:00+00:00   \n",
       "\n",
       "                         LossDate          Loss Region Peril    Weight LobId  \\\n",
       "0       2020-05-30 00:00:00+00:00  2.250859e+10     EU    WS  0.000004     5   \n",
       "1       2020-05-13 00:00:00+00:00  2.197889e+10     EU    WS  0.000004     5   \n",
       "2       2020-11-10 00:00:00+00:00  2.160561e+10     EU    WS  0.000004     5   \n",
       "3       2020-11-10 00:00:00+00:00  2.092983e+10     EU    WS  0.000004     5   \n",
       "4       2020-05-13 00:00:00+00:00  1.993543e+10     EU    WS  0.000004     5   \n",
       "...                           ...           ...    ...   ...       ...   ...   \n",
       "7381503 2020-10-21 00:00:00+00:00  1.870107e-03     EU    WS  0.000004     2   \n",
       "7381504 2020-12-14 00:00:00+00:00  1.669423e-03     EU    WS  0.000004     1   \n",
       "7381505 2020-10-25 00:00:00+00:00  8.474226e-04     EU    WS  0.000004     2   \n",
       "7381506 2020-12-16 00:00:00+00:00  6.278728e-04     EU    WS  0.000004     1   \n",
       "7381507 2020-01-16 00:00:00+00:00  3.690754e-04     EU    WS  0.000004     5   \n",
       "\n",
       "        LobName  \n",
       "0          SPER  \n",
       "1          SPER  \n",
       "2          SPER  \n",
       "3          SPER  \n",
       "4          SPER  \n",
       "...         ...  \n",
       "7381503    AUTO  \n",
       "7381504     AGR  \n",
       "7381505    AUTO  \n",
       "7381506     AGR  \n",
       "7381507    SPER  \n",
       "\n",
       "[7381508 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Prompt the user to enter the path to the Parquet file\n",
    "file_path2 = input(\"Enter the file path: \")\n",
    "\n",
    "# Ensure the file path is treated as a raw string and strip any extra quotes\n",
    "file_path2= r\"{}\".format(file_path.strip('\"'))\n",
    "\n",
    "# Read the Parquet file into a PyArrow Table\n",
    "table2 = pq.read_table(file_path)\n",
    "\n",
    "# Convert the PyArrow Table to a Pandas DataFrame\n",
    "df2 = table.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PeriodId   EventId  EventDate   LossDate       Loss Region Peril    Weight  \\\n",
      "0      6257  53858484 2020-01-14 2020-01-14   0.979822     EU    WS  0.000004   \n",
      "1      6257  53858484 2020-01-14 2020-01-14   0.000871     EU    WS  0.000004   \n",
      "2      6257  53858484 2020-01-14 2020-01-14  11.855758     EU    WS  0.000004   \n",
      "3      6257  53858484 2020-01-14 2020-01-14   8.406023     EU    WS  0.000004   \n",
      "4      6257  53858484 2020-01-14 2020-01-14   0.043087     EU    WS  0.000004   \n",
      "\n",
      "    Loss_sum  \n",
      "0   0.979822  \n",
      "1   0.000871  \n",
      "2  11.855758  \n",
      "3   8.406023  \n",
      "4   0.043087  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Prompt the user to enter the path to the Parquet file\n",
    "file_path = input(\"Enter the file path: \")\n",
    "\n",
    "# Ensure the file path is treated as a raw string and strip any extra quotes\n",
    "file_path = r\"{}\".format(file_path.strip('\"'))\n",
    "\n",
    "# Open the Parquet file\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 10000  # Adjust the batch size as needed\n",
    "\n",
    "# Iterate over the file in batches\n",
    "for batch in parquet_file.iter_batches(batch_size):\n",
    "    # Convert the batch to a Pandas DataFrame\n",
    "    df_batch = batch.to_pandas()\n",
    "    \n",
    "    # Print the header of the batch\n",
    "    print(df_batch.head())\n",
    "    \n",
    "    # Optionally, process the batch here\n",
    "    # ...\n",
    "\n",
    "    # Break after the first batch if you only want to print the header of the first batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema (data types):\n",
      "EPType           object\n",
      "Loss            float64\n",
      "ReturnPeriod    float64\n",
      "LobId            object\n",
      "LobName          object\n",
      "dtype: object\n",
      "\n",
      "Columns:\n",
      "Index(['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName'], dtype='object')\n",
      "\n",
      "Decimal places for floating-point numbers:\n",
      "Loss: 10 decimal places\n",
      "ReturnPeriod: 1 decimal places\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "  EPType          Loss  ReturnPeriod LobId LobName\n",
      "0    OEP  8.639885e+08       10000.0     2    AUTO\n",
      "1    OEP  6.749918e+08        5000.0     2    AUTO\n",
      "2    OEP  3.778001e+08        1000.0     2    AUTO\n",
      "3    OEP  2.847455e+08         500.0     2    AUTO\n",
      "4    OEP  2.064693e+08         250.0     2    AUTO\n"
     ]
    }
   ],
   "source": [
    "# Display the schema (data types) of the DataFrame\n",
    "print(\"Schema (data types):\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display the columns of the DataFrame\n",
    "print(\"\\nColumns:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Display the number of decimal places for floating-point numbers\n",
    "print(\"\\nDecimal places for floating-point numbers:\")\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    decimal_places = df[col].apply(lambda x: len(str(x).split('.')[1]) if '.' in str(x) else 0).max()\n",
    "    print(f\"{col}: {decimal_places} decimal places\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nFirst few rows of the DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
