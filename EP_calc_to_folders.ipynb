{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EP LOB GU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 402653184 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_calc_to_folders.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_calc_to_folders.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m final_table_2 \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mconcat_tables(final_grouped_table_2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_calc_to_folders.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Perform final grouping and sorting\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_calc_to_folders.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m f_grouped_table_1 \u001b[39m=\u001b[39m final_table_1\u001b[39m.\u001b[39;49mgroup_by([\u001b[39m'\u001b[39;49m\u001b[39mEventId\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mPeriodId\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mLobName\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49maggregate([(\u001b[39m'\u001b[39;49m\u001b[39mMax_Loss\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_calc_to_folders.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m f_grouped_table_2 \u001b[39m=\u001b[39m final_table_2\u001b[39m.\u001b[39mgroup_by([\u001b[39m'\u001b[39m\u001b[39mEventId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPeriodId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39maggregate([(\u001b[39m'\u001b[39m\u001b[39mSum_Loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_calc_to_folders.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m sorted_final_table_1 \u001b[39m=\u001b[39m f_grouped_table_1\u001b[39m.\u001b[39msort_by([(\u001b[39m'\u001b[39m\u001b[39mMax_Loss_sum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdescending\u001b[39m\u001b[39m'\u001b[39m)])\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\table.pxi:6509\u001b[0m, in \u001b[0;36mpyarrow.lib.TableGroupBy.aggregate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\acero.py:403\u001b[0m, in \u001b[0;36m_group_by\u001b[1;34m(table, aggregates, keys, use_threads)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_group_by\u001b[39m(table, aggregates, keys, use_threads\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    399\u001b[0m     decl \u001b[39m=\u001b[39m Declaration\u001b[39m.\u001b[39mfrom_sequence([\n\u001b[0;32m    400\u001b[0m         Declaration(\u001b[39m\"\u001b[39m\u001b[39mtable_source\u001b[39m\u001b[39m\"\u001b[39m, TableSourceNodeOptions(table)),\n\u001b[0;32m    401\u001b[0m         Declaration(\u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m, AggregateNodeOptions(aggregates, keys\u001b[39m=\u001b[39mkeys))\n\u001b[0;32m    402\u001b[0m     ])\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m decl\u001b[39m.\u001b[39;49mto_table(use_threads\u001b[39m=\u001b[39;49muse_threads)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\_acero.pyx:590\u001b[0m, in \u001b[0;36mpyarrow._acero.Declaration.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 402653184 failed"
     ]
    }
   ],
   "source": [
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# # Initialize an empty list to store the results\n",
    "# final_grouped_table_1 = []\n",
    "# final_grouped_table_2 = []\n",
    "\n",
    "# # Process each Parquet file individually\n",
    "# for file in parquet_files:\n",
    "#     # Read the Parquet file into a PyArrow Table\n",
    "#     table = pq.read_table(file)\n",
    "    \n",
    "#     # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, and LobName\n",
    "#     grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'LobName','LocationId']).aggregate([('Loss', 'max')])\n",
    "#     grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId','LocationId', 'LobName', 'Max_Loss'])\n",
    "#     grouped_table_1 = grouped_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Max_Loss', 'sum')])\n",
    "#     grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'Max_Loss'])\n",
    "#     # grouped_table_1 = grouped_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Max_Loss_sum', 'max')])\n",
    "#     # grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'Max_Loss'])\n",
    "\n",
    "\n",
    "    \n",
    "#     # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "#     grouped_table_2 = table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "#     grouped_table_2 = grouped_table_2.rename_columns(['EventId', 'PeriodId', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "#     # Append the grouped Table to the final_grouped_tables list\n",
    "#     final_grouped_table_1.append(grouped_table_1)\n",
    "#     final_grouped_table_2.append(grouped_table_2)\n",
    "\n",
    "# # Concatenate all grouped tables\n",
    "# final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "# final_table_2 = pa.concat_tables(final_grouped_table_2)\n",
    "\n",
    "# # Perform final grouping and sorting\n",
    "# f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Max_Loss', 'sum')])\n",
    "# f_grouped_table_2 = final_table_2.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "# sorted_final_table_1 = f_grouped_table_1.sort_by([('Max_Loss_sum', 'descending')])\n",
    "# sorted_final_table_2 = f_grouped_table_2.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# # The Tables are now ready for the next instructions\n",
    "# dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "# dataframe_1_aep = sorted_final_table_2.to_pandas()\n",
    "\n",
    "# # Filter and assign to variables if not empty\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR'].empty:\n",
    "#     daf_AGR_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO'].empty:\n",
    "#     daf_AUTO_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM'].empty:\n",
    "#     daf_COM_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND'].empty:\n",
    "#     daf_IND_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER'].empty:\n",
    "#     daf_SPER_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST'].empty:\n",
    "#     daf_FRST_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST']\n",
    "\n",
    "# if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH'].empty:\n",
    "#     daf_GLH_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR'].empty:\n",
    "#     daf_AGR_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO'].empty:\n",
    "#     daf_AUTO_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM'].empty:\n",
    "#     daf_COM_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND'].empty:\n",
    "#     daf_IND_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER'].empty:\n",
    "#     daf_SPER_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST'].empty:\n",
    "#     daf_FRST_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST']\n",
    "\n",
    "# if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH'].empty:\n",
    "#     daf_GLH_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OEP file path: D:\\RISHIN\\ILC_TEST\\final_dataframe_1_oep.parquet\n",
      "Final AEP file path: D:\\RISHIN\\ILC_TEST\\final_dataframe_1_aep.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_table_1 = []\n",
    "final_grouped_table_2 = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, LobName, and LocationId\n",
    "    grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'LobName', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'EventDate', 'Sum_Loss'])\n",
    "    #grouped_table_1 = grouped_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "   # grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'Max_Loss'])\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table_2 = table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_2 = grouped_table_2.rename_columns(['EventId', 'PeriodId', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "    # Write intermediate results to disk\n",
    "    pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "    pq.write_table(grouped_table_2, os.path.join(processing_folder_path, f'grouped_table_2_{os.path.basename(file)}'))\n",
    "\n",
    "# Read all intermediate files and concatenate them\n",
    "intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "intermediate_files_2 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_2_')]\n",
    "\n",
    "final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "final_grouped_table_2 = [pq.read_table(f) for f in intermediate_files_2]\n",
    "\n",
    "final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "final_table_2 = pa.concat_tables(final_grouped_table_2)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "f_grouped_table_2 = final_table_2.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "sorted_final_table_2 = f_grouped_table_2.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "dataframe_1_oep = sorted_final_table_1.to_pandas()\n",
    "dataframe_1_aep = sorted_final_table_2.to_pandas()\n",
    "\n",
    "# you can delete the steps below later\n",
    "\n",
    "# Write the final concatenated files to disk\n",
    "final_oep_path = os.path.join(output_folder_path, 'final_dataframe_1_oep.parquet')\n",
    "final_aep_path = os.path.join(output_folder_path, 'final_dataframe_1_aep.parquet')\n",
    "pq.write_table(sorted_final_table_1, final_oep_path)\n",
    "pq.write_table(sorted_final_table_2, final_aep_path)\n",
    "\n",
    "# Delete all non-concatenated files\n",
    "for f in intermediate_files_1 + intermediate_files_2:\n",
    "    os.remove(f)\n",
    "\n",
    "print(f'Final OEP file path: {final_oep_path}')\n",
    "print(f'Final AEP file path: {final_aep_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Delete the final concatenated files\n",
    "# if os.path.exists(final_oep_path):\n",
    "#     os.remove(final_oep_path)\n",
    "# if os.path.exists(final_aep_path):\n",
    "#     os.remove(final_aep_path)\n",
    "\n",
    "# # Delete all files in the processing folder\n",
    "# for f in os.listdir(processing_folder_path):\n",
    "#     file_path = os.path.join(processing_folder_path, f)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         os.remove(file_path)\n",
    "\n",
    "# # Delete the processing folder if it is empty\n",
    "# if not os.listdir(processing_folder_path):\n",
    "#     os.rmdir(processing_folder_path)\n",
    "\n",
    "# print(f'Deleted final files and cleaned up processing folder: {processing_folder_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and assign to variables if not empty\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR'].empty:\n",
    "    daf_AGR_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AGR']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO'].empty:\n",
    "    daf_AUTO_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'AUTO']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM'].empty:\n",
    "    daf_COM_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'COM']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND'].empty:\n",
    "    daf_IND_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'IND']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER'].empty:\n",
    "    daf_SPER_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'SPER']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST'].empty:\n",
    "    daf_FRST_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'FRST']\n",
    "\n",
    "if not dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH'].empty:\n",
    "    daf_GLH_oep = dataframe_1_oep[dataframe_1_oep['LobName'] == 'GLH']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR'].empty:\n",
    "    daf_AGR_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AGR']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO'].empty:\n",
    "    daf_AUTO_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'AUTO']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM'].empty:\n",
    "    daf_COM_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'COM']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND'].empty:\n",
    "    daf_IND_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'IND']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER'].empty:\n",
    "    daf_SPER_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'SPER']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST'].empty:\n",
    "    daf_FRST_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'FRST']\n",
    "\n",
    "if not dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH'].empty:\n",
    "    daf_GLH_aep = dataframe_1_aep[dataframe_1_aep['LobName'] == 'GLH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>LobName</th>\n",
       "      <th>Sum_Loss_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53895970</td>\n",
       "      <td>207557</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.250859e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53891598</td>\n",
       "      <td>149148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.197889e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53893301</td>\n",
       "      <td>4312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.160561e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53893301</td>\n",
       "      <td>154312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.092983e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53891598</td>\n",
       "      <td>199148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.993543e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374263</th>\n",
       "      <td>53876702</td>\n",
       "      <td>165828</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>1.870107e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374264</th>\n",
       "      <td>53865125</td>\n",
       "      <td>130081</td>\n",
       "      <td>AGR</td>\n",
       "      <td>1.669423e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374265</th>\n",
       "      <td>53881107</td>\n",
       "      <td>82560</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>8.474226e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374266</th>\n",
       "      <td>53872002</td>\n",
       "      <td>212031</td>\n",
       "      <td>AGR</td>\n",
       "      <td>6.278728e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374267</th>\n",
       "      <td>53885692</td>\n",
       "      <td>249127</td>\n",
       "      <td>SPER</td>\n",
       "      <td>3.690754e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7374268 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EventId  PeriodId LobName  Sum_Loss_sum\n",
       "0        53895970    207557    SPER  2.250859e+10\n",
       "1        53891598    149148    SPER  2.197889e+10\n",
       "2        53893301      4312    SPER  2.160561e+10\n",
       "3        53893301    154312    SPER  2.092983e+10\n",
       "4        53891598    199148    SPER  1.993543e+10\n",
       "...           ...       ...     ...           ...\n",
       "7374263  53876702    165828    AUTO  1.870107e-03\n",
       "7374264  53865125    130081     AGR  1.669423e-03\n",
       "7374265  53881107     82560    AUTO  8.474226e-04\n",
       "7374266  53872002    212031     AGR  6.278728e-04\n",
       "7374267  53885692    249127    SPER  3.690754e-04\n",
       "\n",
       "[7374268 rows x 4 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_1_oep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>LobName</th>\n",
       "      <th>Sum_Loss_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53895970</td>\n",
       "      <td>207557</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.250859e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53891598</td>\n",
       "      <td>149148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.197889e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53893301</td>\n",
       "      <td>4312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.160561e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53893301</td>\n",
       "      <td>154312</td>\n",
       "      <td>SPER</td>\n",
       "      <td>2.092983e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53891598</td>\n",
       "      <td>199148</td>\n",
       "      <td>SPER</td>\n",
       "      <td>1.993543e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374263</th>\n",
       "      <td>53876702</td>\n",
       "      <td>165828</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>1.870107e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374264</th>\n",
       "      <td>53865125</td>\n",
       "      <td>130081</td>\n",
       "      <td>AGR</td>\n",
       "      <td>1.669423e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374265</th>\n",
       "      <td>53881107</td>\n",
       "      <td>82560</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>8.474226e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374266</th>\n",
       "      <td>53872002</td>\n",
       "      <td>212031</td>\n",
       "      <td>AGR</td>\n",
       "      <td>6.278728e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374267</th>\n",
       "      <td>53885692</td>\n",
       "      <td>249127</td>\n",
       "      <td>SPER</td>\n",
       "      <td>3.690754e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7374268 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EventId  PeriodId LobName  Sum_Loss_sum\n",
       "0        53895970    207557    SPER  2.250859e+10\n",
       "1        53891598    149148    SPER  2.197889e+10\n",
       "2        53893301      4312    SPER  2.160561e+10\n",
       "3        53893301    154312    SPER  2.092983e+10\n",
       "4        53891598    199148    SPER  1.993543e+10\n",
       "...           ...       ...     ...           ...\n",
       "7374263  53876702    165828    AUTO  1.870107e-03\n",
       "7374264  53865125    130081     AGR  1.669423e-03\n",
       "7374265  53881107     82560    AUTO  8.474226e-04\n",
       "7374266  53872002    212031     AGR  6.278728e-04\n",
       "7374267  53885692    249127    SPER  3.690754e-04\n",
       "\n",
       "[7374268 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_1_aep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import decimal\n",
    "\n",
    "\n",
    "# def process_and_save_parquet_2(dataframe_1_oep,dataframe_1_aep ,parquet_file_path, speriod, samples):\n",
    "#     dataframe_2 = dataframe_1_oep.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Max_Loss_sum': 'max'})\n",
    "#     dataframe_2.rename(columns={'Max_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "#     dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "#     dataframe_3 = dataframe_1_aep.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "#     dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "#     dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "#     dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "#     dataframe_2['cumrate'] = dataframe_2['rate'].cumsum()\n",
    "#     dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "#     dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * \n",
    "#                               (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "#     dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "#     dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "#     dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "#     dataframe_3['cumrate'] = dataframe_3['rate'].cumsum()\n",
    "#     dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "#     dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * \n",
    "#                               (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "#     dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "#     dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "#     rps_values = [float(x) for x in [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]]\n",
    "#     fdataframe_2 = pd.DataFrame()\n",
    "#     fdataframe_3 = pd.DataFrame()\n",
    "#     decimal_places = 8\n",
    "\n",
    "#     for value in rps_values:\n",
    "#         rounded_value = round(value, decimal_places)\n",
    "#         fdataframe_2 = pd.concat([fdataframe_2, dataframe_2[np.round(dataframe_2['RPs'], decimal_places) == rounded_value]])\n",
    "#         fdataframe_3 = pd.concat([fdataframe_3, dataframe_3[np.round(dataframe_3['RPs'], decimal_places) == rounded_value]])\n",
    "\n",
    "#     fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "#     fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "#     lobname_to_lobid = {\n",
    "#         'AGR': 1,\n",
    "#         'AUTO': 2,\n",
    "#         'COM': 3,\n",
    "#         'IND': 4,\n",
    "#         'SPER': 5,\n",
    "#         'FRST': 6,\n",
    "#         'GLH': 7\n",
    "#     }\n",
    "\n",
    "#     fdataframe_2['LobId'] = fdataframe_2['LobName'].map(lobname_to_lobid)\n",
    "#     fdataframe_3['LobId'] = fdataframe_3['LobName'].map(lobname_to_lobid)\n",
    "\n",
    "#     # Cast LobId to Decimal with precision 38 and scale 0\n",
    "#     fdataframe_2['LobId'] = fdataframe_2['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "#     fdataframe_3['LobId'] = fdataframe_3['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "\n",
    "#     columns_to_keep_3 = ['RPs', 'LobId', 'LobName']\n",
    "#     columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "#     melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "#                                     var_name='EPType', value_name='Loss')\n",
    "#     melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "#     columns_to_keep_2 = ['RPs', 'LobId', 'LobName']\n",
    "#     columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "#     melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "#                                     var_name='EPType', value_name='Loss')\n",
    "#     melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "#     final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "#     final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "#     new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "#     final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "#     final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "#     # Define the schema to match the required Parquet file schema\n",
    "#     schema = pa.schema([\n",
    "#         pa.field('EPType', pa.string(), nullable=True),\n",
    "#         pa.field('Loss', pa.float64(), nullable=True),\n",
    "#         pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "#         pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "#         pa.field('LobName', pa.string(), nullable=True)\n",
    "#     ])\n",
    "\n",
    "#     # Convert DataFrame to Arrow Table with the specified schema\n",
    "#     table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "#     # Save to Parquet\n",
    "#     pq.write_table(table, parquet_file_path)\n",
    "\n",
    "#     print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate \n",
    "import decimal\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_and_save_parquet_2(dataframe_1_oep, parquet_file_path, speriod, samples):\n",
    "    dataframe_2 = dataframe_1_oep.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "    dataframe_3 = dataframe_1_oep.groupby(['PeriodId', 'LobName'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum()\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * \n",
    "                              (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum()\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * \n",
    "                              (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        closest_index_3 = (dataframe_3['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_3]]])\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    fdataframe_2['LobId'] = fdataframe_2['LobName'].map(lobname_to_lobid)\n",
    "    fdataframe_3['LobId'] = fdataframe_3['LobName'].map(lobname_to_lobid)\n",
    "\n",
    "    # Cast LobId to Decimal with precision 38 and scale 0\n",
    "    fdataframe_2['LobId'] = fdataframe_2['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "    fdataframe_3['LobId'] = fdataframe_3['LobId'].apply(lambda x: decimal.Decimal(x).scaleb(-0))\n",
    "\n",
    "    columns_to_keep_3 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "    columns_to_keep_2 = ['RPs', 'LobId', 'LobName']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "                                    var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod', 'LobId', 'LobName']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LobName', pa.string(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_path_1=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_0.parquet')\n",
    "\n",
    "pq_file_path_2=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_1.parquet')\n",
    "\n",
    "pq_file_path_3=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_2.parquet')\n",
    "\n",
    "pq_file_path_4=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_3.parquet')\n",
    "\n",
    "pq_file_path_5=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_4.parquet')\n",
    "\n",
    "pq_file_path_6=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_5.parquet')\n",
    "\n",
    "pq_file_path_7=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_6.parquet'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    process_and_save_parquet_2(daf_AGR_oep, daf_AGR_aep, pq_file_path_1, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_AUTO_oep, daf_AUTO_aep, pq_file_path_2, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_COM_oep, daf_COM_aep, pq_file_path_3, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_IND_oep, daf_IND_aep, pq_file_path_4, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_SPER_oep, daf_SPER_aep, pq_file_path_5, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_FRST_oep, daf_FRST_aep, pq_file_path_6, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_GLH_oep, daf_GLH_aep, pq_file_path_7, speriod, samples)\n",
    "except (NameError, AttributeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for EP portfoilio GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rough delete inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rough delete inside "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_tables = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table = table.group_by(['EventId', 'PeriodId']).aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Rename the aggregated column to Sum_Loss\n",
    "    grouped_table = grouped_table.rename_columns(['EventId', 'PeriodId', 'Sum_Loss'])\n",
    "    \n",
    "    # Append the grouped Table to the final_grouped_tables list\n",
    "    final_grouped_tables.append(grouped_table)\n",
    "\n",
    "# Concatenate all grouped tables\n",
    "final_table = pa.concat_tables(final_grouped_tables)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "final_grouped_table = final_table.group_by(['EventId', 'PeriodId']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table = final_grouped_table.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "# The Table is now ready for the next instructions\n",
    "dataframe_1 = sorted_final_table\n",
    "dataframe_1= dataframe_1.to_pandas()\n",
    "#dataframe_1 = dataframe_1[dataframe_1['LobName'] == 'AUTO']\n",
    "\n",
    "# Initialize dataframe_2 by selecting PeriodId and max(Sum_Loss) grouped by PeriodId\n",
    "dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "\n",
    "# Rename the aggregated column to Max_Loss\n",
    "dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "\n",
    "# Sort dataframe_2 by Max_Loss in descending order\n",
    "dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False)\n",
    "\n",
    "# Initialize dataframe_2 by selecting PeriodId and Sum(Sum_Loss) grouped by PeriodId\n",
    "dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "\n",
    "# Rename the aggregated column to Sum_Loss\n",
    "dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "\n",
    "# Sort dataframe_3 by S_sum_Loss in descending order\n",
    "dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False)\n",
    "\n",
    "#dataframe_2['Max_Loss'] = dataframe_2['Max_Loss'].round(5)\n",
    "\n",
    "dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "\n",
    "# Calculate the cumulative rate column and round to 6 decimal places\n",
    "dataframe_2['cumrate'] = dataframe_2['rate'].cumsum()\n",
    "\n",
    "# Calculate the RPs column and round to 6 decimal places\n",
    "dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the TCE_OEP_1 column and round to 6 decimal places\n",
    "dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * \n",
    "                          (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "\n",
    "# Calculate the TCE_OEP_2 column and round to 6 decimal places\n",
    "dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "\n",
    "\n",
    "# Calculate the TCE_OEP_Final column and round to 6 decimal places\n",
    "dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "#dataframe_3['S_Sum_Loss'] = dataframe_3['S_Sum_Loss'].round(5)\n",
    "\n",
    "# Calculate the rate column and round to 6 decimal places\n",
    "dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "\n",
    "# Calculate the cumulative rate column and round to 6 decimal places\n",
    "dataframe_3['cumrate'] = dataframe_3['rate'].cumsum()\n",
    "\n",
    "# Calculate the RPs column and round to 6 decimal places\n",
    "dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "\n",
    "\n",
    "# Calculate the TCE_AEP_1 column and round to 6 decimal places\n",
    "dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * \n",
    "                          (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "\n",
    "# Calculate the cumulative sum up to the previous row and multiply by the current row's RPs, then round to 6 decimal places\n",
    "dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "\n",
    "# Calculate the TCE_AEP_Final column and round to 6 decimal places\n",
    "dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "# Define the list of RPs values to filter and convert them to float\n",
    "rps_values = [float(x) for x in [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]]\n",
    "\n",
    "# Initialize an empty DataFrame to store the filtered results\n",
    "fdataframe_2 = pd.DataFrame()\n",
    "fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "# Define the number of decimal places to round to\n",
    "decimal_places = 8\n",
    "\n",
    "# Loop through each value in rps_values and filter the DataFrames\n",
    "for value in rps_values:\n",
    "    rounded_value = round(value, decimal_places)\n",
    "    fdataframe_2 = pd.concat([fdataframe_2, dataframe_2[np.round(dataframe_2['RPs'], decimal_places) == rounded_value]])\n",
    "    fdataframe_3 = pd.concat([fdataframe_3, dataframe_3[np.round(dataframe_3['RPs'], decimal_places) == rounded_value]])\n",
    "\n",
    "\n",
    "fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP','TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "fdataframe_2.rename(columns={ 'Max_Loss': 'OEP','TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "\n",
    "# Define the columns to be used in the new DataFrame for fdataframe_3\n",
    "columns_to_keep_3 = ['RPs']\n",
    "columns_to_melt_3 = [ 'AEP','TCE-AEP']\n",
    "\n",
    "# Melt fdataframe_3 to reshape it\n",
    "melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, \n",
    "                                var_name='EPType', value_name='Loss')\n",
    "\n",
    "# Rename columns to match the desired output\n",
    "melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "# Define the columns to be used in the new DataFrame for fdataframe_2\n",
    "columns_to_keep_2 = ['RPs']\n",
    "columns_to_melt_2 = [ 'OEP','TCE-OEP']\n",
    "\n",
    "# Melt fdataframe_2 to reshape it\n",
    "melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, \n",
    "                                var_name='EPType', value_name='Loss')\n",
    "\n",
    "# Rename columns to match the desired output\n",
    "melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "final_df_EP_Portfolio_GU = pd.concat([ final_df_2,final_df_3], ignore_index=True)\n",
    "\n",
    "\n",
    "# Define the new order for EPType\n",
    "new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "\n",
    "# Update the EPType column to the new order\n",
    "final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by EPType and then by ReturnPeriod in descending order within each EPType\n",
    "final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'EP', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Portfolio_GU_0.parquet')\n",
    "\n",
    "# Save final_df as a Parquet file\n",
    "final_df_EP_Portfolio_GU.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for stats LOB GU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "\n",
    "country = 'BE'\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GU_0.parquet')\n",
    "aggregated_tables_lob_stats = []\n",
    "\n",
    "# Define the mapping of LobName to LobId\n",
    "lobname_to_lobid = {\n",
    "    'AGR': 1,\n",
    "    'AUTO': 2,\n",
    "    'COM': 3,\n",
    "    'IND': 4,\n",
    "    'SPER': 5,\n",
    "    'FRST': 6,\n",
    "    'GLH': 7\n",
    "}\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file):\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select(['LobName', 'AAL'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        aggregated_tables_lob_stats.append(grouped)\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Check if any tables were aggregated\n",
    "if not aggregated_tables_lob_stats:\n",
    "    print(\"No tables were aggregated. Please check the input files.\")\n",
    "else:\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table = pa.concat_tables(aggregated_tables_lob_stats)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('LobId', pa.decimal128(38)),\n",
    "        pa.field('LobName', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for STATS Portfolio GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aggregated_tables = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "    grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Calculate AAL\n",
    "    loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "    aal = loss_sum / speriod / samples\n",
    "    aal_array = pa.array(aal)\n",
    "    grouped = grouped.append_column('AAL', aal_array)\n",
    "    \n",
    "    # Select only the necessary columns\n",
    "    grouped = grouped.select(['LobName', 'AAL'])\n",
    "    \n",
    "    # Append the grouped Table to the list\n",
    "    aggregated_tables.append(grouped)\n",
    "\n",
    "# Concatenate all the grouped Tables\n",
    "final_table = pa.concat_tables(aggregated_tables)\n",
    "\n",
    "# Convert the final table to a Pandas DataFrame\n",
    "final_df = final_table.to_pandas()\n",
    "\n",
    "# Sum all the AAL values without grouping by LobName\n",
    "total_aal = final_df['AAL'].sum()\n",
    "\n",
    "# Create a DataFrame with the specified columns\n",
    "final_df_STATS_Portfolio = pd.DataFrame({\n",
    "    'AAL': [total_aal],\n",
    "    'Std': [np.nan],\n",
    "    'CV': [np.nan],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GU_0.parquet')\n",
    "final_df_STATS_Portfolio.to_parquet(parquet_file_path, index=False)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLT GU Lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "\n",
    "# Define the schema\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(folder_path, 'intermediate_results')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Cast columns to the desired types\n",
    "        table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "        table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "        table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "        table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "        table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "        table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "        table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "        table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "        table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "        table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "        \n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "# Perform the final group by and aggregation\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "# Rename the aggregated column\n",
    "final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "# Save the final table to a Parquet file\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "os.makedirs(main_folder_path, exist_ok=True)\n",
    "parquet_file_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "# Reorder the columns in the desired order\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "# Save the final table to a Parquet file\n",
    "pq.write_table(final_grouped_table, parquet_file_path)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Delete intermediate files\n",
    "for file in intermediate_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Remove the intermediate directory\n",
    "try:\n",
    "    os.rmdir(intermediate_dir)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {intermediate_dir}\")\n",
    "except OSError:\n",
    "    print(f\"Directory not empty or other error: {intermediate_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLT Portfolio GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\Rough\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "# Flush memory at the beginning\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "gc.collect()\n",
    "\n",
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_G.parquet')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = [\"PeriodId\", \"EventId\", \"EventDate\", \"LossDate\", \"Region\", \"Peril\", \"Weight\"]\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    parquet_file = pq.ParquetFile(file)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "# Perform the final group by and aggregation\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "# Rename the aggregated column\n",
    "final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "# Convert PeriodId and EventId to strings\n",
    "final_grouped_table = final_grouped_table.set_column(\n",
    "    final_grouped_table.schema.get_field_index('PeriodId'),\n",
    "    'PeriodId',\n",
    "    final_grouped_table.column('PeriodId').cast(pa.string())\n",
    ")\n",
    "final_grouped_table = final_grouped_table.set_column(\n",
    "    final_grouped_table.schema.get_field_index('EventId'),\n",
    "    'EventId',\n",
    "    final_grouped_table.column('EventId').cast(pa.string())\n",
    ")\n",
    "\n",
    "# Define the schema\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Loss', pa.float64(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Region', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Peril', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Weight', pa.float64(), nullable=True, metadata={'field_id': '-1'})\n",
    "])\n",
    "\n",
    "# Convert the table to the specified schema\n",
    "final_grouped_table = pa.Table.from_arrays(\n",
    "    [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Write the table to a Parquet file with the specified schema\n",
    "parquet_file_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GU_0.parquet')\n",
    "final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "pq.write_table(final_grouped_table, parquet_file_path)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Delete intermediate files\n",
    "for file in intermediate_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Remove the intermediate directory\n",
    "try:\n",
    "    os.rmdir(intermediate_dir)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {intermediate_dir}\")\n",
    "except OSError:\n",
    "    print(f\"Directory not empty or other error: {intermediate_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES_GR'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_folder_path = input(\"Enter the output folder path: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATS GR lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GR_0.parquet')\n",
    "aggregated_tables_lob_stats = []\n",
    "\n",
    "# Define the mapping of LobName to LobId\n",
    "lobname_to_lobid = {\n",
    "    'AGR': 1,\n",
    "    'AUTO': 2,\n",
    "    'COM': 3,\n",
    "    'IND': 4,\n",
    "    'SPER': 5,\n",
    "    'FRST': 6,\n",
    "    'GLH': 7\n",
    "}\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file):\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select(['LobName', 'AAL'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        aggregated_tables_lob_stats.append(grouped)\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Check if any tables were aggregated\n",
    "if not aggregated_tables_lob_stats:\n",
    "    print(\"No tables were aggregated. Please check the input files.\")\n",
    "else:\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table = pa.concat_tables(aggregated_tables_lob_stats)\n",
    "\n",
    "    # Group the final Table again to ensure all groups are combined\n",
    "    final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "    # Sort the final grouped Table by 'AAL' in descending order\n",
    "    final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "    # Convert the final grouped Table to a Pandas DataFrame\n",
    "    final_df = final_grouped.to_pandas()\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "\n",
    "    final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "    # Define the columns with NaN values for 'Std' and 'CV'\n",
    "    final_df_STATS_Lob['Std'] = np.nan\n",
    "    final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "    # Reorder the columns to match the specified format\n",
    "    final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "        pa.field('LobId', pa.decimal128(38)),\n",
    "        pa.field('LobName', pa.string())\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for STATS Portfolio GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aggregated_tables = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "    grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Calculate AAL\n",
    "    loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "    aal = loss_sum / speriod / samples\n",
    "    aal_array = pa.array(aal)\n",
    "    grouped = grouped.append_column('AAL', aal_array)\n",
    "    \n",
    "    # Select only the necessary columns\n",
    "    grouped = grouped.select(['LobName', 'AAL'])\n",
    "    \n",
    "    # Append the grouped Table to the list\n",
    "    aggregated_tables.append(grouped)\n",
    "\n",
    "# Concatenate all the grouped Tables\n",
    "final_table = pa.concat_tables(aggregated_tables)\n",
    "\n",
    "# Convert the final table to a Pandas DataFrame\n",
    "final_df = final_table.to_pandas()\n",
    "\n",
    "# Sum all the AAL values without grouping by LobName\n",
    "total_aal = final_df['AAL'].sum()\n",
    "\n",
    "# Create a DataFrame with the specified columns\n",
    "final_df_STATS_Portfolio = pd.DataFrame({\n",
    "    'AAL': [total_aal],\n",
    "    'Std': [np.nan],\n",
    "    'CV': [np.nan],\n",
    "})\n",
    "\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "\n",
    "# Define the file path for the Parquet file\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GR_0.parquet')\n",
    "final_df_STATS_Portfolio.to_parquet(parquet_file_path, index=False)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLT lob GR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the schema\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(folder_path, 'intermediate_results')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Cast columns to the desired types\n",
    "        table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "        table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "        table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "        table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "        table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "        table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "        table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "        table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "        table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "        table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "        \n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "# Perform the final group by and aggregation\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "# Rename the aggregated column\n",
    "final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "# Save the final table to a Parquet file\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "os.makedirs(main_folder_path, exist_ok=True)\n",
    "parquet_file_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GR_0.parquet')\n",
    "\n",
    "# Reorder the columns in the desired order\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "# Save the final table to a Parquet file\n",
    "pq.write_table(final_grouped_table, parquet_file_path)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Delete intermediate files\n",
    "for file in intermediate_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Remove the intermediate directory\n",
    "try:\n",
    "    os.rmdir(intermediate_dir)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {intermediate_dir}\")\n",
    "except OSError:\n",
    "    print(f\"Directory not empty or other error: {intermediate_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLT Portfolio GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "# Flush memory at the beginning\n",
    "main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "gc.collect()\n",
    "\n",
    "# Directory to store intermediate results\n",
    "intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_G.parquet')\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "group_by_columns = [\"PeriodId\", \"EventId\", \"EventDate\", \"LossDate\", \"Region\", \"Peril\", \"Weight\"]\n",
    "\n",
    "# Process each Parquet file in chunks and write intermediate results to disk\n",
    "for i, file in enumerate(parquet_files):\n",
    "    parquet_file = pq.ParquetFile(file)\n",
    "    for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "        intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "        pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "# Read intermediate results and combine them\n",
    "intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "# Perform the final group by and aggregation\n",
    "final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "# Rename the aggregated column\n",
    "final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "# Convert PeriodId and EventId to strings\n",
    "final_grouped_table = final_grouped_table.set_column(\n",
    "    final_grouped_table.schema.get_field_index('PeriodId'),\n",
    "    'PeriodId',\n",
    "    final_grouped_table.column('PeriodId').cast(pa.string())\n",
    ")\n",
    "final_grouped_table = final_grouped_table.set_column(\n",
    "    final_grouped_table.schema.get_field_index('EventId'),\n",
    "    'EventId',\n",
    "    final_grouped_table.column('EventId').cast(pa.string())\n",
    ")\n",
    "\n",
    "# Define the schema\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Loss', pa.float64(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Region', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Peril', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Weight', pa.float64(), nullable=True, metadata={'field_id': '-1'})\n",
    "])\n",
    "\n",
    "# Convert the table to the specified schema\n",
    "final_grouped_table = pa.Table.from_arrays(\n",
    "    [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Write the table to a Parquet file with the specified schema\n",
    "parquet_file_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GR_0.parquet')\n",
    "final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "pq.write_table(final_grouped_table, parquet_file_path)\n",
    "print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n",
    "# Delete intermediate files\n",
    "for file in intermediate_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Remove the intermediate directory\n",
    "try:\n",
    "    os.rmdir(intermediate_dir)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {intermediate_dir}\")\n",
    "except OSError:\n",
    "    print(f\"Directory not empty or other error: {intermediate_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_0_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_0_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_0_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_10_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_10_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_10_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_11_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_11_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_11_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_12_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_12_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_12_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_13_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_13_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_13_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_14_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_14_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_14_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_15_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_15_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_15_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_1_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_1_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_1_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_2_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_2_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_2_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_3_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_3_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_3_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_4_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_4_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_4_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_5_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_5_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_5_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_6_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_6_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_6_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_7_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_7_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_7_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_8_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_8_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_8_300.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_9_100.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_9_200.parquet',\n",
       " 'D:\\\\RISHIN\\\\13_ILC_TASK1\\\\input\\\\PARQUET_FILES_GR\\\\PLT_9_300.parquet']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
