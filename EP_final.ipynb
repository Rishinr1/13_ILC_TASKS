{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()\n",
    "\n",
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path_gr = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES_GR'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files_gr = [os.path.join(folder_path_gr, f) for f in os.listdir(folder_path_gr) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_folder_and_files(folder_path):\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete all files inside the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        # Delete the folder itself\n",
    "        os.rmdir(folder_path)\n",
    "        print(f'Successfully deleted the folder: {folder_path}')\n",
    "    else:\n",
    "        print(f'The folder {folder_path} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for EP  portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_parquet_files(parquet_files, export_path, filter_string, lob_id, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Filter the table based on the filter_string\n",
    "        table = table.filter(pc.equal(table['LobName'], filter_string))\n",
    "        # Skip if the filtered table is empty\n",
    "        if len(table) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_LOB_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_LOB_GU['EPType'] = pd.Categorical(final_df_EP_LOB_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_LOB_GU = final_df_EP_LOB_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Add LobID and LobName columns\n",
    "    final_df_EP_LOB_GU['LobID'] = lob_id\n",
    "    final_df_EP_LOB_GU['LobName'] = filter_string\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('LobID', pa.string(), nullable=True),\n",
    "        pa.field('LobName', pa.string(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_LOB_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n"
     ]
    }
   ],
   "source": [
    "export_path =os.path.join(main_folder_path, 'EP', 'Lob','GU')\n",
    "parquet_file_path_AUTO = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "rps_values = [10000, 5000, 1000, 500, 250, 200, 100, 50, 25, 10, 5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT FOR GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_0.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_1.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_2.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_3.parquet\n",
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_4.parquet\n",
      "Error processing FRST: Must pass at least one table\n",
      "Error processing GLH: Must pass at least one table\n"
     ]
    }
   ],
   "source": [
    "export_path_gr =os.path.join(main_folder_path, 'EP', 'Lob','GR')\n",
    "parquet_file_path_AUTO = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_1.parquet')\n",
    "parquet_file_path_AGR = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_0.parquet')\n",
    "parquet_file_path_COM = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_2.parquet')\n",
    "parquet_file_path_IND = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_3.parquet')\n",
    "parquet_file_path_SPER = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_4.parquet')\n",
    "parquet_file_path_FRST= os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_5.parquet')\n",
    "parquet_file_path_GLH = os.path.join(export_path_gr, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Lob_GR_6.parquet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_gr, export_path, 'AGR', 1, speriod, samples, rps_values, parquet_file_path_AGR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AGR: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_gr, export_path, 'AUTO', 2, speriod, samples, rps_values, parquet_file_path_AUTO)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing AUTO: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_gr, export_path, 'COM', 3, speriod, samples, rps_values, parquet_file_path_COM)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing COM: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_gr, export_path, 'IND', 4, speriod, samples, rps_values, parquet_file_path_IND)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing IND: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_gr, export_path, 'SPER', 5, speriod, samples, rps_values, parquet_file_path_SPER)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing SPER: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_gr, export_path, 'FRST', 6, speriod, samples, rps_values, parquet_file_path_FRST)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing FRST: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_parquet_files(parquet_files_gr, export_path, 'GLH', 7, speriod, samples, rps_values, parquet_file_path_GLH)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing GLH: {e}\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\n"
     ]
    }
   ],
   "source": [
    "processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "delete_folder_and_files(processing_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values,parquet_file_path):\n",
    "    processing_folder_path = os.path.join(main_folder_path, 'processing')\n",
    "    partial_folder_path = os.path.join(processing_folder_path, 'partial')\n",
    "    concatenated_folder_path = os.path.join(processing_folder_path, 'concatenated')\n",
    "\n",
    "    os.makedirs(processing_folder_path, exist_ok=True)\n",
    "    os.makedirs(partial_folder_path, exist_ok=True)\n",
    "    os.makedirs(concatenated_folder_path, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    final_grouped_table_1 = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "\n",
    "        grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Loss', 'sum')])\n",
    "        grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'EventDate', 'Sum_Loss'])\n",
    "    \n",
    "        # Write intermediate results to disk\n",
    "        pq.write_table(grouped_table_1, os.path.join(partial_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "\n",
    "    # Read all intermediate files and concatenate them\n",
    "    intermediate_files_1 = [os.path.join(partial_folder_path, f) for f in os.listdir(partial_folder_path) if f.startswith('grouped_table_1_')]\n",
    "\n",
    "    final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "\n",
    "    final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "\n",
    "    # Perform final grouping and sorting\n",
    "    f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'EventDate']).aggregate([('Sum_Loss', 'sum')])\n",
    "    sorted_final_table_1 = f_grouped_table_1.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "    pq.write_table(sorted_final_table_1, os.path.join(concatenated_folder_path, 'final_grouped_table_1.parquet'))\n",
    "\n",
    "    # Delete all non-concatenated files\n",
    "    for f in intermediate_files_1:\n",
    "        os.remove(f)\n",
    "    \n",
    "    dataframe_1 = sorted_final_table_1.to_pandas()\n",
    "\n",
    "    dataframe_2 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'max'})\n",
    "    dataframe_2.rename(columns={'Sum_Loss_sum': 'Max_Loss'}, inplace=True)\n",
    "    dataframe_2 = dataframe_2.sort_values(by='Max_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_2['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_2['cumrate'] = dataframe_2['rate'].cumsum().round(6)\n",
    "    dataframe_2['RPs'] = (1 / dataframe_2['cumrate'])\n",
    "    dataframe_2['TCE_OEP_1'] = ((dataframe_2['Max_Loss'] - dataframe_2['Max_Loss'].shift(-1)) * (dataframe_2['cumrate'] + dataframe_2['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_2['TCE_OEP_2'] = (dataframe_2['TCE_OEP_1'].shift().cumsum() * dataframe_2['RPs'])\n",
    "    dataframe_2['TCE_OEP_Final'] = (dataframe_2['TCE_OEP_2'] + dataframe_2['Max_Loss'])\n",
    "\n",
    "    dataframe_3 = dataframe_1.groupby(['PeriodId'], as_index=False).agg({'Sum_Loss_sum': 'sum'})\n",
    "    dataframe_3.rename(columns={'Sum_Loss_sum': 'S_Sum_Loss'}, inplace=True)\n",
    "    dataframe_3 = dataframe_3.sort_values(by='S_Sum_Loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dataframe_3['rate'] = (1 / (speriod * samples))\n",
    "    dataframe_3['cumrate'] = dataframe_3['rate'].cumsum().round(6)\n",
    "    dataframe_3['RPs'] = (1 / dataframe_3['cumrate'])\n",
    "    dataframe_3['TCE_AEP_1'] = ((dataframe_3['S_Sum_Loss'] - dataframe_3['S_Sum_Loss'].shift(-1)) * (dataframe_3['cumrate'] + dataframe_3['cumrate'].shift(-1)) * 0.5)\n",
    "    dataframe_3['TCE_AEP_2'] = (dataframe_3['TCE_AEP_1'].shift().cumsum() * dataframe_3['RPs'])\n",
    "    dataframe_3['TCE_AEP_Final'] = (dataframe_3['TCE_AEP_2'] + dataframe_3['S_Sum_Loss'])\n",
    "\n",
    "    fdataframe_2 = pd.DataFrame()\n",
    "    fdataframe_3 = pd.DataFrame()\n",
    "\n",
    "    for value in rps_values:\n",
    "        closest_index_2 = (dataframe_2['RPs'] - value).abs().idxmin()\n",
    "        fdataframe_2 = pd.concat([fdataframe_2, dataframe_2.loc[[closest_index_2]]])\n",
    "        fdataframe_3 = pd.concat([fdataframe_3, dataframe_3.loc[[closest_index_2]]])\n",
    "\n",
    "    fdataframe_2.rename(columns={'Max_Loss': 'OEP', 'TCE_OEP_Final': 'TCE-OEP'}, inplace=True)\n",
    "    columns_to_keep_2 = ['RPs']\n",
    "    columns_to_melt_2 = ['OEP', 'TCE-OEP']\n",
    "    melted_df_2 = fdataframe_2.melt(id_vars=columns_to_keep_2, value_vars=columns_to_melt_2, var_name='EPType', value_name='Loss')\n",
    "    melted_df_2.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_2 = melted_df_2[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    fdataframe_3.rename(columns={'S_Sum_Loss': 'AEP', 'TCE_AEP_Final': 'TCE-AEP'}, inplace=True)\n",
    "    columns_to_keep_3 = ['RPs']\n",
    "    columns_to_melt_3 = ['AEP', 'TCE-AEP']\n",
    "    melted_df_3 = fdataframe_3.melt(id_vars=columns_to_keep_3, value_vars=columns_to_melt_3, var_name='EPType', value_name='Loss')\n",
    "    melted_df_3.rename(columns={'RPs': 'ReturnPeriod'}, inplace=True)\n",
    "    final_df_3 = melted_df_3[['EPType', 'Loss', 'ReturnPeriod']]\n",
    "\n",
    "    final_df_EP_Portfolio_GU = pd.concat([final_df_2, final_df_3], ignore_index=True)\n",
    "    new_ep_type_order = [\"OEP\", \"AEP\", \"TCE-OEP\", \"TCE-AEP\"]\n",
    "    final_df_EP_Portfolio_GU['EPType'] = pd.Categorical(final_df_EP_Portfolio_GU['EPType'], categories=new_ep_type_order, ordered=True)\n",
    "    final_df_EP_Portfolio_GU = final_df_EP_Portfolio_GU.sort_values(by=['EPType', 'ReturnPeriod'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    # Convert DataFrame to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_pandas(final_df_EP_Portfolio_GU, schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "export_path =os.path.join(main_folder_path, 'EP', 'Portfolio','GU')\n",
    "parquet_file_path = os.path.join(export_path, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GU_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files, export_path, speriod, samples, rps_values, parquet_file_path)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\EP\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "export_path_GR =os.path.join(main_folder_path,'EP','Portfolio','GR')\n",
    "parquet_file_path_GR = os.path.join(export_path_GR, 'ILC2024_EUWS_PLA_WI_EP_BE_EUR_EP_Portfolio_GR_0.parquet')\n",
    "try:\n",
    "    process_parquet_files_Port(parquet_files_gr, export_path_GR, speriod, samples, rps_values, parquet_file_path_GR)\n",
    "except (NameError, AttributeError,ValueError) as e:\n",
    "    print(f\"Error processing : {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the folder: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\partial\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\\concatenated\n",
      "Successfully deleted the folder: D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\processing\n"
     ]
    }
   ],
   "source": [
    "delete_folder_and_files(partial_folder_path)\n",
    "delete_folder_and_files(concatenated_folder_path)\n",
    "delete_folder_and_files(processing_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for stats LOB GU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "def process_lob_stats(parquet_files, parquet_file_path):\n",
    "    aggregated_tables_lob_stats = []\n",
    "\n",
    "    # Define the mapping of LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file):\n",
    "            # Read the Parquet file into a PyArrow Table\n",
    "            table = pq.read_table(file)\n",
    "            \n",
    "            # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "            grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "            \n",
    "            # Calculate AAL\n",
    "            loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "            aal = loss_sum / speriod / samples\n",
    "            aal_array = pa.array(aal)\n",
    "            grouped = grouped.append_column('AAL', aal_array)\n",
    "            \n",
    "            # Select only the necessary columns\n",
    "            grouped = grouped.select(['LobName', 'AAL'])\n",
    "            \n",
    "            # Append the grouped Table to the list\n",
    "            aggregated_tables_lob_stats.append(grouped)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Check if any tables were aggregated\n",
    "    if not aggregated_tables_lob_stats:\n",
    "        print(\"No tables were aggregated. Please check the input files.\")\n",
    "    else:\n",
    "        # Concatenate all the grouped Tables\n",
    "        final_table = pa.concat_tables(aggregated_tables_lob_stats)\n",
    "\n",
    "        # Group the final Table again to ensure all groups are combined\n",
    "        final_grouped = final_table.group_by('LobName').aggregate([('AAL', 'sum')])\n",
    "\n",
    "        # Sort the final grouped Table by 'AAL' in descending order\n",
    "        final_grouped = final_grouped.sort_by([('AAL_sum', 'descending')])\n",
    "\n",
    "        # Convert the final grouped Table to a Pandas DataFrame\n",
    "        final_df = final_grouped.to_pandas()\n",
    "\n",
    "        # Map LobName to LobId\n",
    "        final_df['LobId'] = final_df['LobName'].map(lobname_to_lobid).apply(lambda x: Decimal(x))\n",
    "\n",
    "        final_df_STATS_Lob = final_df.rename(columns={'AAL_sum': 'AAL'})\n",
    "\n",
    "        # Define the columns with NaN values for 'Std' and 'CV'\n",
    "        final_df_STATS_Lob['Std'] = np.nan\n",
    "        final_df_STATS_Lob['CV'] = np.nan\n",
    "\n",
    "        # Reorder the columns to match the specified format\n",
    "        final_df_STATS_Lob = final_df_STATS_Lob[['AAL', 'Std', 'CV', 'LobId', 'LobName']]\n",
    "\n",
    "        # Define the desired schema\n",
    "        desired_schema = pa.schema([\n",
    "            pa.field('AAL', pa.float64()),\n",
    "            pa.field('Std', pa.float64()),\n",
    "            pa.field('CV', pa.float64()),\n",
    "            pa.field('LobId', pa.decimal128(38)),\n",
    "            pa.field('LobName', pa.string())\n",
    "        ])\n",
    "\n",
    "        # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "        final_table_STATS_Lob = pa.Table.from_pandas(final_df_STATS_Lob, schema=desired_schema)\n",
    "        pq.write_table(final_table_STATS_Lob, parquet_file_path)\n",
    "        print(f\"Parquet file saved successfully at {parquet_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOB GU STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GU_0.parquet')\n",
    "process_lob_stats(parquet_files, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOB GR STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Lob_GR_0.parquet')\n",
    "process_lob_stats(parquet_files, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Portfolio STATS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_portfolio_stats(parquet_files, export_path):\n",
    "    aggregated_tables = []\n",
    "\n",
    "    # Process each Parquet file individually\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file into a PyArrow Table\n",
    "        table = pq.read_table(file)\n",
    "        \n",
    "        # Perform the aggregation: sum the Loss column grouped by LobName\n",
    "        grouped = table.group_by('LobName').aggregate([('Loss', 'sum')])\n",
    "        \n",
    "        # Calculate AAL\n",
    "        loss_sum = grouped.column('Loss_sum').to_numpy()\n",
    "        aal = loss_sum / speriod / samples\n",
    "        aal_array = pa.array(aal)\n",
    "        grouped = grouped.append_column('AAL', aal_array)\n",
    "        \n",
    "        # Select only the necessary columns\n",
    "        grouped = grouped.select(['LobName', 'AAL'])\n",
    "        \n",
    "        # Append the grouped Table to the list\n",
    "        aggregated_tables.append(grouped)\n",
    "\n",
    "    # Concatenate all the grouped Tables\n",
    "    final_table = pa.concat_tables(aggregated_tables)\n",
    "\n",
    "    # Convert the final table to a Pandas DataFrame\n",
    "    final_df = final_table.to_pandas()\n",
    "\n",
    "    # Sum all the AAL values without grouping by LobName\n",
    "    total_aal = final_df['AAL'].sum()\n",
    "\n",
    "    # Create a DataFrame with the specified columns\n",
    "    final_df_STATS_Portfolio = pd.DataFrame({\n",
    "        'AAL': [total_aal],\n",
    "        'Std': [np.nan],\n",
    "        'CV': [np.nan],\n",
    "    })\n",
    "\n",
    "    # Define the desired schema\n",
    "    desired_schema = pa.schema([\n",
    "        pa.field('AAL', pa.float64()),\n",
    "        pa.field('Std', pa.float64()),\n",
    "        pa.field('CV', pa.float64()),\n",
    "    ])\n",
    "\n",
    "    # Convert the DataFrame back to a PyArrow Table with the desired schema\n",
    "    final_table_STATS_Portfolio = pa.Table.from_pandas(final_df_STATS_Portfolio, schema=desired_schema)\n",
    "    pq.write_table(final_table_STATS_Portfolio, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "#GU\n",
    "\n",
    "parquet_file_path = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GU_0.parquet')\n",
    "process_portfolio_stats(parquet_files, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\STATS\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_STATS_Portfolio_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "parquet_file_path_gr = os.path.join(main_folder_path, 'STATS', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_STATS_Portfolio_GR_0.parquet')\n",
    "process_portfolio_stats(parquet_files_gr, parquet_file_path_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW FOR PLT LOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PLT_lob(parquet_files, export_path):\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            \n",
    "            # Cast columns to the desired types\n",
    "            table = table.set_column(table.schema.get_field_index('PeriodId'), 'PeriodId', pa.compute.cast(table['PeriodId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventId'), 'EventId', pa.compute.cast(table['EventId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('EventDate'), 'EventDate', pa.compute.cast(table['EventDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('LossDate'), 'LossDate', pa.compute.cast(table['LossDate'], pa.timestamp('ms', tz='UTC')))\n",
    "            table = table.set_column(table.schema.get_field_index('Loss'), 'Loss', pa.compute.cast(table['Loss'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('Region'), 'Region', pa.compute.cast(table['Region'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Peril'), 'Peril', pa.compute.cast(table['Peril'], pa.string()))\n",
    "            table = table.set_column(table.schema.get_field_index('Weight'), 'Weight', pa.compute.cast(table['Weight'], pa.float64()))\n",
    "            table = table.set_column(table.schema.get_field_index('LobId'), 'LobId', pa.compute.cast(table['LobId'], pa.decimal128(38, 0)))\n",
    "            table = table.set_column(table.schema.get_field_index('LobName'), 'LobName', pa.compute.cast(table['LobName'], pa.string()))\n",
    "            \n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss_sum_sum', 'descending')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    # Reorder the columns in the desired order\n",
    "    final_grouped_table = final_grouped_table.select(ordered_columns)\n",
    "\n",
    "    # Save the final table to a Parquet file\n",
    "        # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "        # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "    try:\n",
    "        pq.write_table(final_grouped_table, export_path)\n",
    "        print(f\"Parquet file saved successfully at {export_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True),\n",
    "    pa.field('Loss', pa.float64(), nullable=True),\n",
    "    pa.field('Region', pa.string(), nullable=True),\n",
    "    pa.field('Peril', pa.string(), nullable=True),\n",
    "    pa.field('Weight', pa.float64(), nullable=True),\n",
    "    pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "    pa.field('LobName', pa.string(), nullable=True)\n",
    "])\n",
    "\n",
    "group_by_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "ordered_columns = ['PeriodId', 'EventId', 'EventDate', 'LossDate', 'Loss', 'Region', 'Peril', 'Weight', 'LobId', 'LobName']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GU_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\RISHIN\\\\TESTING FOLDER\\\\ILC_TEST_5\\\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Lob\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Lob_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "export_path = os.path.join(main_folder_path, 'PLT', 'Lob', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Lob_GR_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files_gr, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLT Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PLT_portfolio(parquet_files, export_path):\n",
    "    # Flush memory at the beginning\n",
    "    gc.collect()\n",
    "\n",
    "    # Directory to store intermediate results\n",
    "    intermediate_dir = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', 'intermediate_results')\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # Process each Parquet file in chunks and write intermediate results to disk\n",
    "    for i, file in enumerate(parquet_files):\n",
    "        parquet_file = pq.ParquetFile(file)\n",
    "        for j, batch in enumerate(parquet_file.iter_batches()):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            grouped_table = table.group_by(group_by_columns).aggregate([('Loss', 'sum')])\n",
    "            intermediate_file = os.path.join(intermediate_dir, f\"intermediate_{i}_{j}.parquet\")\n",
    "            pq.write_table(grouped_table, intermediate_file)\n",
    "\n",
    "    # Read intermediate results and combine them\n",
    "    intermediate_files = [os.path.join(intermediate_dir, f) for f in os.listdir(intermediate_dir) if f.endswith('.parquet')]\n",
    "    intermediate_tables = [pq.read_table(file) for file in intermediate_files]\n",
    "    combined_grouped_table = pa.concat_tables(intermediate_tables)\n",
    "\n",
    "    # Perform the final group by and aggregation\n",
    "    final_grouped_table = combined_grouped_table.group_by(group_by_columns).aggregate([('Loss_sum', 'sum')])\n",
    "\n",
    "    # Rename the aggregated column\n",
    "    final_grouped_table = final_grouped_table.rename_columns(group_by_columns + ['Loss'])\n",
    "\n",
    "    # Convert PeriodId and EventId to strings\n",
    "    final_grouped_table = final_grouped_table.set_column(\n",
    "        final_grouped_table.schema.get_field_index('PeriodId'),\n",
    "        'PeriodId',\n",
    "        final_grouped_table.column('PeriodId').cast(pa.string())\n",
    "    )\n",
    "    final_grouped_table = final_grouped_table.set_column(\n",
    "        final_grouped_table.schema.get_field_index('EventId'),\n",
    "        'EventId',\n",
    "        final_grouped_table.column('EventId').cast(pa.string())\n",
    "    )\n",
    "\n",
    "    # Convert the table to the specified schema\n",
    "    final_grouped_table = pa.Table.from_arrays(\n",
    "        [final_grouped_table.column(name).cast(schema.field(name).type) for name in schema.names],\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "        # Delete intermediate files\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Remove the intermediate directory\n",
    "    try:\n",
    "        os.rmdir(intermediate_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {intermediate_dir}\")\n",
    "    except OSError:\n",
    "        print(f\"Directory not empty or other error: {intermediate_dir}\")\n",
    "\n",
    "\n",
    "    # Write the table to a Parquet file with the specified schema\n",
    "    final_grouped_table = final_grouped_table.sort_by([('Loss', 'descending')])\n",
    "\n",
    "\n",
    "    pq.write_table(final_grouped_table, export_path)\n",
    "    print(f\"Parquet file saved successfully at {export_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field('PeriodId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventId', pa.decimal128(38, 0), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('EventDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('LossDate', pa.timestamp('ms', tz='UTC'), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Loss', pa.float64(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Region', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Peril', pa.string(), nullable=True, metadata={'field_id': '-1'}),\n",
    "    pa.field('Weight', pa.float64(), nullable=True, metadata={'field_id': '-1'})\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GU\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GU_0.parquet\n"
     ]
    }
   ],
   "source": [
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GU_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully at D:\\RISHIN\\TESTING FOLDER\\ILC_TEST_5\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\\PLT\\Portfolio\\GR\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_PLT_Portfolio_GR_0.parquet\n"
     ]
    }
   ],
   "source": [
    "export_path = os.path.join(main_folder_path, 'PLT', 'Portfolio', 'GR', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_PLT_Portfolio_GR_0.parquet')\n",
    "\n",
    "process_PLT_lob(parquet_files_gr, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
