{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Function to flush the cache\n",
    "def flush_cache():\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\TESTING FOLDER\\ILC-TEST_2\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EP LOB GU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OEP file path: D:\\RISHIN\\TESTING FOLDER\\ILC-TEST_2\\processedfiles\\final_dataframe_1_oep.parquet\n",
      "Final AEP file path: D:\\RISHIN\\TESTING FOLDER\\ILC-TEST_2\\processedfiles\\final_dataframe_1_aep.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "processing_folder_path = os.path.join(output_folder_path, 'processing')\n",
    "os.makedirs(processing_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "final_grouped_table_1 = []\n",
    "final_grouped_table_2 = []\n",
    "\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: max the Loss column grouped by EventId, PeriodId, LobName, and LocationId\n",
    "    grouped_table_1 = table.group_by(['EventId', 'PeriodId', 'LobName', 'LocationId']).aggregate([('Loss', 'mean')])\n",
    "    grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'LocationId', 'Max_Loss'])\n",
    "    grouped_table_1 = grouped_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Max_Loss', 'sum')])\n",
    "    grouped_table_1 = grouped_table_1.rename_columns(['EventId', 'PeriodId', 'LobName', 'Max_Loss'])\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table_2 = table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "    grouped_table_2 = grouped_table_2.rename_columns(['EventId', 'PeriodId', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "    # Write intermediate results to disk\n",
    "    pq.write_table(grouped_table_1, os.path.join(processing_folder_path, f'grouped_table_1_{os.path.basename(file)}'))\n",
    "    pq.write_table(grouped_table_2, os.path.join(processing_folder_path, f'grouped_table_2_{os.path.basename(file)}'))\n",
    "\n",
    "# Read all intermediate files and concatenate them\n",
    "intermediate_files_1 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_1_')]\n",
    "intermediate_files_2 = [os.path.join(processing_folder_path, f) for f in os.listdir(processing_folder_path) if f.startswith('grouped_table_2_')]\n",
    "\n",
    "final_grouped_table_1 = [pq.read_table(f) for f in intermediate_files_1]\n",
    "final_grouped_table_2 = [pq.read_table(f) for f in intermediate_files_2]\n",
    "\n",
    "final_table_1 = pa.concat_tables(final_grouped_table_1)\n",
    "final_table_2 = pa.concat_tables(final_grouped_table_2)\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "f_grouped_table_1 = final_table_1.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Max_Loss', 'sum')])\n",
    "f_grouped_table_2 = final_table_2.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table_1 = f_grouped_table_1.sort_by([('Max_Loss_sum', 'descending')])\n",
    "sorted_final_table_2 = f_grouped_table_2.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "dataframe_1_oep = sorted_final_table_1\n",
    "dataframe_1_aep = sorted_final_table_2\n",
    "\n",
    "# you can delete the steps below later\n",
    "\n",
    "# Create the processedfiles folder within the output folder\n",
    "processed_files_folder = os.path.join(output_folder_path, 'processedfiles')\n",
    "os.makedirs(processed_files_folder, exist_ok=True)\n",
    "\n",
    "# Define the paths for the final concatenated files\n",
    "final_oep_path = os.path.join(processed_files_folder, 'final_dataframe_1_oep.parquet')\n",
    "final_aep_path = os.path.join(processed_files_folder, 'final_dataframe_1_aep.parquet')\n",
    "\n",
    "# Write the final concatenated files to disk\n",
    "pq.write_table(dataframe_1_oep, final_oep_path)\n",
    "pq.write_table(dataframe_1_aep, final_aep_path)\n",
    "\n",
    "# Delete all non-concatenated files\n",
    "for f in intermediate_files_1 + intermediate_files_2:\n",
    "    os.remove(f)\n",
    "# Delete the 'process' folder\n",
    "if os.path.exists(processing_folder_path):\n",
    "    shutil.rmtree(processing_folder_path)\n",
    "\n",
    "\n",
    "print(f'Final OEP file path: {final_oep_path}')\n",
    "\n",
    "print(f'Final AEP file path: {final_aep_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "grouped_dataframes_folder_path = os.path.join(output_folder_path, 'grouped_dataframes')\n",
    "intermediate_grouping_folder_path = os.path.join(output_folder_path, 'intermediate_grouping')\n",
    "\n",
    "# Create grouped dataframes and intermediate grouping folders\n",
    "os.makedirs(grouped_dataframes_folder_path, exist_ok=True)\n",
    "os.makedirs(intermediate_grouping_folder_path, exist_ok=True)\n",
    "\n",
    "# Load the parquet files into pyarrow tables\n",
    "dataframe_1_oep = pq.read_table(final_oep_path)\n",
    "dataframe_1_aep = pq.read_table(final_aep_path)\n",
    "\n",
    "# Function to filter and assign to variables if not empty\n",
    "def filter_and_assign(table, column_name, value):\n",
    "    filtered_table = table.filter(pa.compute.equal(table[column_name], value))\n",
    "    if filtered_table.num_rows > 0:\n",
    "        return filtered_table\n",
    "    return None\n",
    "\n",
    "# Function to save table in chunks\n",
    "def save_table_in_chunks(table, folder_path, filename_prefix, chunk_size=100000):\n",
    "    num_chunks = (table.num_rows + chunk_size - 1) // chunk_size\n",
    "    for i in range(num_chunks):\n",
    "        chunk = table.slice(i * chunk_size, chunk_size)\n",
    "        file_path = os.path.join(folder_path, f\"{filename_prefix}_chunk_{i}.parquet\")\n",
    "        pq.write_table(chunk, file_path)\n",
    "\n",
    "# Filter and assign to variables\n",
    "daf_AGR_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'AGR')\n",
    "daf_AUTO_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'AUTO')\n",
    "daf_COM_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'COM')\n",
    "daf_IND_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'IND')\n",
    "daf_SPER_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'SPER')\n",
    "daf_FRST_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'FRST')\n",
    "daf_GLH_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'GLH')\n",
    "\n",
    "daf_AGR_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'AGR')\n",
    "daf_AUTO_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'AUTO')\n",
    "daf_COM_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'COM')\n",
    "daf_IND_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'IND')\n",
    "daf_SPER_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'SPER')\n",
    "daf_FRST_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'FRST')\n",
    "daf_GLH_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'GLH')\n",
    "\n",
    "# Save filtered tables in chunks if they are not None\n",
    "if daf_AGR_oep is not None:\n",
    "    save_table_in_chunks(daf_AGR_oep, intermediate_grouping_folder_path, 'partial_daf_AGR_oep')\n",
    "if daf_AUTO_oep is not None:\n",
    "    save_table_in_chunks(daf_AUTO_oep, intermediate_grouping_folder_path, 'partial_daf_AUTO_oep')\n",
    "if daf_COM_oep is not None:\n",
    "    save_table_in_chunks(daf_COM_oep, intermediate_grouping_folder_path, 'partial_daf_COM_oep')\n",
    "if daf_IND_oep is not None:\n",
    "    save_table_in_chunks(daf_IND_oep, intermediate_grouping_folder_path, 'partial_daf_IND_oep')\n",
    "if daf_SPER_oep is not None:\n",
    "    save_table_in_chunks(daf_SPER_oep, intermediate_grouping_folder_path, 'partial_daf_SPER_oep')\n",
    "if daf_FRST_oep is not None:\n",
    "    save_table_in_chunks(daf_FRST_oep, intermediate_grouping_folder_path, 'partial_daf_FRST_oep')\n",
    "if daf_GLH_oep is not None:\n",
    "    save_table_in_chunks(daf_GLH_oep, intermediate_grouping_folder_path, 'partial_daf_GLH_oep')\n",
    "\n",
    "if daf_AGR_aep is not None:\n",
    "    save_table_in_chunks(daf_AGR_aep, intermediate_grouping_folder_path, 'partial_daf_AGR_aep')\n",
    "if daf_AUTO_aep is not None:\n",
    "    save_table_in_chunks(daf_AUTO_aep, intermediate_grouping_folder_path, 'partial_daf_AUTO_aep')\n",
    "if daf_COM_aep is not None:\n",
    "    save_table_in_chunks(daf_COM_aep, intermediate_grouping_folder_path, 'partial_daf_COM_aep')\n",
    "if daf_IND_aep is not None:\n",
    "    save_table_in_chunks(daf_IND_aep, intermediate_grouping_folder_path, 'partial_daf_IND_aep')\n",
    "if daf_SPER_aep is not None:\n",
    "    save_table_in_chunks(daf_SPER_aep, intermediate_grouping_folder_path, 'partial_daf_SPER_aep')\n",
    "if daf_FRST_aep is not None:\n",
    "    save_table_in_chunks(daf_FRST_aep, intermediate_grouping_folder_path, 'partial_daf_FRST_aep')\n",
    "if daf_GLH_aep is not None:\n",
    "    save_table_in_chunks(daf_GLH_aep, intermediate_grouping_folder_path, 'partial_daf_GLH_aep')\n",
    "\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def align_schema(table, target_schema):\n",
    "    \"\"\"\n",
    "    Align the schema of the table to the target schema by renaming columns.\n",
    "    \"\"\"\n",
    "    table_schema = table.schema\n",
    "    new_fields = []\n",
    "    for field in target_schema:\n",
    "        if field.name in table_schema.names:\n",
    "            new_fields.append(table.field(field.name))\n",
    "        else:\n",
    "            new_fields.append(pa.field(field.name, field.type))\n",
    "    return table.rename_columns([field.name for field in new_fields])\n",
    "\n",
    "def concatenate_and_save(folder_path, output_folder_path):\n",
    "    grouped_files = {}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.parquet'):\n",
    "            prefix = '_'.join(file.split('_')[:-2])\n",
    "            if prefix not in grouped_files:\n",
    "                grouped_files[prefix] = []\n",
    "            grouped_files[prefix].append(file)\n",
    "    \n",
    "    for prefix, files in grouped_files.items():\n",
    "        tables = []\n",
    "        target_schema = None\n",
    "        for file in files:\n",
    "            table = pq.read_table(os.path.join(folder_path, file))\n",
    "            if target_schema is None:\n",
    "                target_schema = table.schema\n",
    "            else:\n",
    "                table = align_schema(table, target_schema)\n",
    "            tables.append(table)\n",
    "        if tables:\n",
    "            concatenated_table = pa.concat_tables(tables)\n",
    "            pq.write_table(concatenated_table, os.path.join(output_folder_path, f\"{prefix}.parquet\"))\n",
    "\n",
    "def safe_concatenate_and_save(intermediate_path, grouped_path):\n",
    "    try:\n",
    "        concatenate_and_save(intermediate_path, grouped_path)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "\n",
    "# Create grouped dataframes and intermediate grouping folders\n",
    "grouped_dataframes_folder_path = os.path.join(output_folder_path, 'grouped_dataframes')\n",
    "intermediate_grouping_folder_path = os.path.join(output_folder_path, 'intermediate_grouping')\n",
    "os.makedirs(grouped_dataframes_folder_path, exist_ok=True)\n",
    "os.makedirs(intermediate_grouping_folder_path, exist_ok=True)\n",
    "\n",
    "# Load the parquet files into pyarrow tables\n",
    "dataframe_1_oep = pq.read_table(final_oep_path)\n",
    "dataframe_1_aep = pq.read_table(final_aep_path)\n",
    "\n",
    "# Function to filter and assign to variables if not empty\n",
    "def filter_and_assign(table, column_name, value):\n",
    "    filtered_table = table.filter(pa.compute.equal(table[column_name], value))\n",
    "    if filtered_table.num_rows > 0:\n",
    "        return filtered_table\n",
    "    return None\n",
    "\n",
    "# Function to save table in chunks\n",
    "def save_table_in_chunks(table, folder_path, filename_prefix, chunk_size=100000):\n",
    "    num_chunks = (table.num_rows + chunk_size - 1) // chunk_size\n",
    "    for i in range(num_chunks):\n",
    "        chunk = table.slice(i * chunk_size, chunk_size)\n",
    "        file_path = os.path.join(folder_path, f\"{filename_prefix}_chunk_{i}.parquet\")\n",
    "        pq.write_table(chunk, file_path)\n",
    "\n",
    "# Filter and assign to variables\n",
    "daf_AGR_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'AGR')\n",
    "daf_AUTO_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'AUTO')\n",
    "daf_COM_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'COM')\n",
    "daf_IND_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'IND')\n",
    "daf_SPER_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'SPER')\n",
    "daf_FRST_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'FRST')\n",
    "daf_GLH_oep = filter_and_assign(dataframe_1_oep, 'LobName', 'GLH')\n",
    "\n",
    "daf_AGR_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'AGR')\n",
    "daf_AUTO_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'AUTO')\n",
    "daf_COM_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'COM')\n",
    "daf_IND_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'IND')\n",
    "daf_SPER_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'SPER')\n",
    "daf_FRST_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'FRST')\n",
    "daf_GLH_aep = filter_and_assign(dataframe_1_aep, 'LobName', 'GLH')\n",
    "\n",
    "# Save filtered tables in chunks if they are not None\n",
    "if daf_AGR_oep is not None:\n",
    "    save_table_in_chunks(daf_AGR_oep, intermediate_grouping_folder_path, 'partial_daf_AGR_oep')\n",
    "if daf_AUTO_oep is not None:\n",
    "    save_table_in_chunks(daf_AUTO_oep, intermediate_grouping_folder_path, 'partial_daf_AUTO_oep')\n",
    "if daf_COM_oep is not None:\n",
    "    save_table_in_chunks(daf_COM_oep, intermediate_grouping_folder_path, 'partial_daf_COM_oep')\n",
    "if daf_IND_oep is not None:\n",
    "    save_table_in_chunks(daf_IND_oep, intermediate_grouping_folder_path, 'partial_daf_IND_oep')\n",
    "if daf_SPER_oep is not None:\n",
    "    save_table_in_chunks(daf_SPER_oep, intermediate_grouping_folder_path, 'partial_daf_SPER_oep')\n",
    "if daf_FRST_oep is not None:\n",
    "    save_table_in_chunks(daf_FRST_oep, intermediate_grouping_folder_path, 'partial_daf_FRST_oep')\n",
    "if daf_GLH_oep is not None:\n",
    "    save_table_in_chunks(daf_GLH_oep, intermediate_grouping_folder_path, 'partial_daf_GLH_oep')\n",
    "\n",
    "if daf_AGR_aep is not None:\n",
    "    save_table_in_chunks(daf_AGR_aep, intermediate_grouping_folder_path, 'partial_daf_AGR_aep')\n",
    "if daf_AUTO_aep is not None:\n",
    "    save_table_in_chunks(daf_AUTO_aep, intermediate_grouping_folder_path, 'partial_daf_AUTO_aep')\n",
    "if daf_COM_aep is not None:\n",
    "    save_table_in_chunks(daf_COM_aep, intermediate_grouping_folder_path, 'partial_daf_COM_aep')\n",
    "if daf_IND_aep is not None:\n",
    "    save_table_in_chunks(daf_IND_aep, intermediate_grouping_folder_path, 'partial_daf_IND_aep')\n",
    "if daf_SPER_aep is not None:\n",
    "    save_table_in_chunks(daf_SPER_aep, intermediate_grouping_folder_path, 'partial_daf_SPER_aep')\n",
    "if daf_FRST_aep is not None:\n",
    "    save_table_in_chunks(daf_FRST_aep, intermediate_grouping_folder_path, 'partial_daf_FRST_aep')\n",
    "if daf_GLH_aep is not None:\n",
    "    save_table_in_chunks(daf_GLH_aep, intermediate_grouping_folder_path, 'partial_daf_GLH_aep')\n",
    "\n",
    "# Call the function to concatenate and save the files\n",
    "safe_concatenate_and_save(intermediate_grouping_folder_path, grouped_dataframes_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete other folders and their contents\n",
    "def delete_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "\n",
    "delete_folder(processed_files_folder)\n",
    "delete_folder(intermediate_grouping_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_and_save_parquet_2(dataframe_1_oep, dataframe_1_aep, parquet_file_path, speriod, samples):\n",
    "    # Group by 'PeriodId' and 'LobName' and aggregate 'Max_Loss_sum' for OEP\n",
    "    grouped_oep = dataframe_1_oep.group_by(['PeriodId', 'LobName']).aggregate([('Max_Loss_sum', 'max')])\n",
    "    grouped_oep = grouped_oep.rename_columns(['PeriodId', 'LobName', 'Max_Loss'])\n",
    "\n",
    "    # Group by 'PeriodId' and 'LobName' and aggregate 'Sum_Loss_sum' for AEP\n",
    "    grouped_aep = dataframe_1_aep.group_by(['PeriodId', 'LobName']).aggregate([('Sum_Loss_sum', 'sum')])\n",
    "    grouped_aep = grouped_aep.rename_columns(['PeriodId', 'LobName', 'S_Sum_Loss'])\n",
    "\n",
    "    # Calculate additional columns for OEP\n",
    "    rate = pa.array([1 / (speriod * samples)] * len(grouped_oep))\n",
    "    cumrate = pa.compute.cumulative_sum(rate)\n",
    "    rps = pa.compute.divide(1, cumrate)\n",
    "    max_loss_shifted = pa.compute.shift(grouped_oep['Max_Loss'], -1)\n",
    "    cumrate_shifted = pa.compute.shift(cumrate, -1)\n",
    "    tce_oep_1 = pa.compute.multiply(pa.compute.subtract(grouped_oep['Max_Loss'], max_loss_shifted),\n",
    "                                    pa.compute.multiply(pa.compute.add(cumrate, cumrate_shifted), 0.5))\n",
    "    tce_oep_2 = pa.compute.multiply(pa.compute.cumulative_sum(pa.compute.shift(tce_oep_1, 1)), rps)\n",
    "    tce_oep_final = pa.compute.add(tce_oep_2, grouped_oep['Max_Loss'])\n",
    "\n",
    "    # Calculate additional columns for AEP\n",
    "    s_sum_loss_shifted = pa.compute.shift(grouped_aep['S_Sum_Loss'], -1)\n",
    "    tce_aep_1 = pa.compute.multiply(pa.compute.subtract(grouped_aep['S_Sum_Loss'], s_sum_loss_shifted),\n",
    "                                    pa.compute.multiply(pa.compute.add(cumrate, cumrate_shifted), 0.5))\n",
    "    tce_aep_2 = pa.compute.multiply(pa.compute.cumulative_sum(pa.compute.shift(tce_aep_1, 1)), rps)\n",
    "    tce_aep_final = pa.compute.add(tce_aep_2, grouped_aep['S_Sum_Loss'])\n",
    "\n",
    "    # Create final tables\n",
    "    final_oep = pa.table([grouped_oep['PeriodId'], grouped_oep['LobName'], grouped_oep['Max_Loss'], tce_oep_final, rps],\n",
    "                         names=['PeriodId', 'LobName', 'OEP', 'TCE-OEP', 'RPs'])\n",
    "    final_aep = pa.table([grouped_aep['PeriodId'], grouped_aep['LobName'], grouped_aep['S_Sum_Loss'], tce_aep_final, rps],\n",
    "                         names=['PeriodId', 'LobName', 'AEP', 'TCE-AEP', 'RPs'])\n",
    "\n",
    "    # Map LobName to LobId\n",
    "    lobname_to_lobid = {\n",
    "        'AGR': 1,\n",
    "        'AUTO': 2,\n",
    "        'COM': 3,\n",
    "        'IND': 4,\n",
    "        'SPER': 5,\n",
    "        'FRST': 6,\n",
    "        'GLH': 7\n",
    "    }\n",
    "    lobid_array = pa.array([lobname_to_lobid[name.as_py()] for name in final_oep['LobName']])\n",
    "    lobid_decimal = pa.array([decimal.Decimal(x).scaleb(-0) for x in lobid_array])\n",
    "\n",
    "    final_oep = final_oep.append_column('LobId', lobid_decimal)\n",
    "    final_aep = final_aep.append_column('LobId', lobid_decimal)\n",
    "\n",
    "    # Melt the tables\n",
    "    melted_oep = pa.table([final_oep['RPs'], final_oep['LobId'], final_oep['LobName'], final_oep['OEP'], final_oep['TCE-OEP']],\n",
    "                          names=['ReturnPeriod', 'LobId', 'LobName', 'OEP', 'TCE-OEP'])\n",
    "    melted_aep = pa.table([final_aep['RPs'], final_aep['LobId'], final_aep['LobName'], final_aep['AEP'], final_aep['TCE-AEP']],\n",
    "                          names=['ReturnPeriod', 'LobId', 'LobName', 'AEP', 'TCE-AEP'])\n",
    "\n",
    "    # Concatenate the final tables\n",
    "    final_table = pa.concat_tables([melted_oep, melted_aep])\n",
    "\n",
    "    # Define the schema to match the required Parquet file schema\n",
    "    schema = pa.schema([\n",
    "        pa.field('EPType', pa.string(), nullable=True),\n",
    "        pa.field('Loss', pa.float64(), nullable=True),\n",
    "        pa.field('ReturnPeriod', pa.float64(), nullable=True),\n",
    "        pa.field('LobId', pa.decimal128(38, 0), nullable=True),\n",
    "        pa.field('LobName', pa.string(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    # Convert to Arrow Table with the specified schema\n",
    "    table = pa.Table.from_arrays([final_table['EPType'], final_table['Loss'], final_table['ReturnPeriod'], final_table['LobId'], final_table['LobName']], schema=schema)\n",
    "\n",
    "    # Save to Parquet\n",
    "    pq.write_table(table, parquet_file_path)\n",
    "\n",
    "    print(f\"Parquet file saved successfully at {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_path_1=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_0.parquet')\n",
    "\n",
    "pq_file_path_2=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_1.parquet')\n",
    "\n",
    "pq_file_path_3=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_2.parquet')\n",
    "\n",
    "pq_file_path_4=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_3.parquet')\n",
    "\n",
    "pq_file_path_5=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_4.parquet')\n",
    "\n",
    "pq_file_path_6=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_5.parquet')\n",
    "\n",
    "pq_file_path_7=os.path.join(main_folder_path, 'EP', 'Lob', 'GU', f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_EP_Lob_GU_6.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow.compute' has no attribute 'shift'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_CALC_to_folders2.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     process_and_save_parquet_2(daf_AGR_oep,daf_AGR_aep, pq_file_path_1, speriod, samples)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_CALC_to_folders2.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cumrate \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39mcumulative_sum(rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m rps \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39mdivide(\u001b[39m1\u001b[39m, cumrate)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m max_loss_shifted \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mcompute\u001b[39m.\u001b[39;49mshift(grouped_oep[\u001b[39m'\u001b[39m\u001b[39mMax_Loss\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m cumrate_shifted \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39mshift(cumrate, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m tce_oep_1 \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39mmultiply(pa\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39msubtract(grouped_oep[\u001b[39m'\u001b[39m\u001b[39mMax_Loss\u001b[39m\u001b[39m'\u001b[39m], max_loss_shifted),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                                 pa\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39mmultiply(pa\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39madd(cumrate, cumrate_shifted), \u001b[39m0.5\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyarrow.compute' has no attribute 'shift'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    process_and_save_parquet_2(daf_AGR_oep,daf_AGR_aep, pq_file_path_1, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_AUTO_oep,daf_AUTO_aep, pq_file_path_2, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_COM_oep,daf_COM_aep, pq_file_path_3, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_IND_oep,daf_IND_aep, pq_file_path_4, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_SPER_oep,daf_SPER_aep, pq_file_path_5, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_FRST_oep,daf_FRST_aep, pq_file_path_6, speriod, samples)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    process_and_save_parquet_2(daf_GLH_oep,daf_GLH_aep, pq_file_path_7, speriod, samples)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
