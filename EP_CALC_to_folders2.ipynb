{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "import gc\n",
    "from decimal import Decimal  # Add this import statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speriod=int(input(\"Enter the simulation period: \"))\n",
    "samples=int(input(\"Enter the number of samples: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the Parquet files\n",
    "folder_path = r'D:\\RISHIN\\13_ILC_TASK1\\input\\PARQUET_FILES'\n",
    "\n",
    "# List all Parquet files in the folder\n",
    "parquet_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = input(\"Enter the output folder path: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully at D:\\RISHIN\\ILC_TEST\\ILC2024_EUWS_PLA_WI_EP_BE_EUR_Losses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if there are any Parquet files in the folder\n",
    "if parquet_files:\n",
    "    # Read the first Parquet file in chunks\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "        # Convert the first batch to a PyArrow Table\n",
    "        table = pa.Table.from_batches([batch])\n",
    "        \n",
    "        # Convert the PyArrow Table to a Pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Extract the first value of LocationName and split it by '_'\n",
    "        location_name = df['LocationName'].iloc[0]\n",
    "        country = location_name.split('_')[0]\n",
    "        \n",
    "        \n",
    "        # Define the main folder path\n",
    "        main_folder_path = os.path.join(output_folder_path, f'ILC2024_EUWS_PLA_WI_EP_{country}_EUR_Losses')\n",
    "        \n",
    "        # Define subfolders\n",
    "        subfolders = ['EP', 'PLT', 'STATS']\n",
    "        nested_folders = ['Lob', 'Portfolio']\n",
    "        innermost_folders = ['GR', 'GU']\n",
    "        \n",
    "        # Create the main folder and subfolders\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            \n",
    "            for nested_folder in nested_folders:\n",
    "                nested_folder_path = os.path.join(subfolder_path, nested_folder)\n",
    "                os.makedirs(nested_folder_path, exist_ok=True)\n",
    "                \n",
    "                for innermost_folder in innermost_folders:\n",
    "                    innermost_folder_path = os.path.join(nested_folder_path, innermost_folder)\n",
    "                    os.makedirs(innermost_folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Folders created successfully at {main_folder_path}\")\n",
    "        break  # Process only the first batch\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EP LOB GU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\EP_CALC_to_folders2.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Process each Parquet file individually\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m parquet_files:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Read the Parquet file into a PyArrow Table\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     table \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39;49mread_table(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RISHIN/13_ILC_TASK1/EP_CALC_to_folders2.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     grouped_table \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39mgroup_by([\u001b[39m'\u001b[39m\u001b[39mEventId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPeriodId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLobName\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39maggregate([(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)])\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[39m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1832\u001b[0m     dataset \u001b[39m=\u001b[39m ParquetFile(\n\u001b[0;32m   1833\u001b[0m         source, read_dictionary\u001b[39m=\u001b[39mread_dictionary,\n\u001b[0;32m   1834\u001b[0m         memory_map\u001b[39m=\u001b[39mmemory_map, buffer_size\u001b[39m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         page_checksum_verification\u001b[39m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1841\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39;49mread(columns\u001b[39m=\u001b[39;49mcolumns, use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[0;32m   1844\u001b[0m                     use_pandas_metadata\u001b[39m=\u001b[39;49muse_pandas_metadata)\n",
      "File \u001b[1;32md:\\RISHIN\\13_ILC_TASK1\\env\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         index_columns \u001b[39m=\u001b[39m [\n\u001b[0;32m   1478\u001b[0m             col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1479\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mdict\u001b[39m)\n\u001b[0;32m   1480\u001b[0m         ]\n\u001b[0;32m   1481\u001b[0m         columns \u001b[39m=\u001b[39m (\n\u001b[0;32m   1482\u001b[0m             \u001b[39mlist\u001b[39m(columns) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(index_columns) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(columns))\n\u001b[0;32m   1483\u001b[0m         )\n\u001b[1;32m-> 1485\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset\u001b[39m.\u001b[39;49mto_table(\n\u001b[0;32m   1486\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_expression,\n\u001b[0;32m   1487\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads\n\u001b[0;32m   1488\u001b[0m )\n\u001b[0;32m   1490\u001b[0m \u001b[39m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[39m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[39mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the results\n",
    "final_grouped_tables = []\n",
    "# Process each Parquet file individually\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Perform the aggregation: sum the Loss column grouped by EventId, PeriodId, and LobName\n",
    "    grouped_table = table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Loss', 'sum')])\n",
    "    \n",
    "    # Rename the aggregated column to Sum_Loss\n",
    "    grouped_table = grouped_table.rename_columns(['EventId', 'PeriodId', 'LobName', 'Sum_Loss'])\n",
    "    \n",
    "    # Append the grouped Table to the final_grouped_tables list\n",
    "    final_grouped_tables.append(grouped_table)\n",
    "\n",
    "# Concatenate all grouped tables\n",
    "final_table = pa.concat_tables(final_grouped_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records: 760\n"
     ]
    }
   ],
   "source": [
    "final_df = final_table.to_pandas()\n",
    "\n",
    "duplicate_records = final_df.duplicated(subset=['EventId', 'PeriodId', 'LobName'], keep=False)\n",
    "\n",
    "num_duplicates = duplicate_records.sum()\n",
    "\n",
    "print(f\"Number of duplicate records: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventId       27633\n",
      "PeriodId     248280\n",
      "LobName           5\n",
      "Sum_Loss    7333970\n",
      "dtype: int64\n",
      "          EventId  PeriodId LobName      Sum_Loss\n",
      "146301   53856487      9495    AUTO  3.171431e+07\n",
      "146361   53884900      3306    AUTO  7.319426e+07\n",
      "146363   53871525      3321    AUTO  3.847942e+07\n",
      "146382   53884900      3306     COM  1.409573e+08\n",
      "146392   53871525      3321     COM  7.672723e+07\n",
      "...           ...       ...     ...           ...\n",
      "7296249  53863133    153629     COM  8.515380e+07\n",
      "7296254  53879975    153637     AGR  2.619929e+05\n",
      "7296269  53877262    153626     IND  3.586264e+04\n",
      "7296270  53877262    153626    SPER  4.707738e+04\n",
      "7296308  53863133    153629    AUTO  2.649277e+07\n",
      "\n",
      "[760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "duplicate_mask = final_df.duplicated(subset=['EventId', 'PeriodId', 'LobName'], keep=False)\n",
    "duplicate_rows_df = final_df[duplicate_mask]\n",
    "\n",
    "print(final_df.nunique())\n",
    "\n",
    "print(duplicate_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>PeriodId</th>\n",
       "      <th>LobName</th>\n",
       "      <th>Sum_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146301</th>\n",
       "      <td>53856487</td>\n",
       "      <td>9495</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>3.171431e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146361</th>\n",
       "      <td>53884900</td>\n",
       "      <td>3306</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>7.319426e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146363</th>\n",
       "      <td>53871525</td>\n",
       "      <td>3321</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>3.847942e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146382</th>\n",
       "      <td>53884900</td>\n",
       "      <td>3306</td>\n",
       "      <td>COM</td>\n",
       "      <td>1.409573e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146392</th>\n",
       "      <td>53871525</td>\n",
       "      <td>3321</td>\n",
       "      <td>COM</td>\n",
       "      <td>7.672723e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296249</th>\n",
       "      <td>53863133</td>\n",
       "      <td>153629</td>\n",
       "      <td>COM</td>\n",
       "      <td>8.515380e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296254</th>\n",
       "      <td>53879975</td>\n",
       "      <td>153637</td>\n",
       "      <td>AGR</td>\n",
       "      <td>2.619929e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296269</th>\n",
       "      <td>53877262</td>\n",
       "      <td>153626</td>\n",
       "      <td>IND</td>\n",
       "      <td>3.586264e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296270</th>\n",
       "      <td>53877262</td>\n",
       "      <td>153626</td>\n",
       "      <td>SPER</td>\n",
       "      <td>4.707738e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296308</th>\n",
       "      <td>53863133</td>\n",
       "      <td>153629</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>2.649277e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>760 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EventId  PeriodId LobName      Sum_Loss\n",
       "146301   53856487      9495    AUTO  3.171431e+07\n",
       "146361   53884900      3306    AUTO  7.319426e+07\n",
       "146363   53871525      3321    AUTO  3.847942e+07\n",
       "146382   53884900      3306     COM  1.409573e+08\n",
       "146392   53871525      3321     COM  7.672723e+07\n",
       "...           ...       ...     ...           ...\n",
       "7296249  53863133    153629     COM  8.515380e+07\n",
       "7296254  53879975    153637     AGR  2.619929e+05\n",
       "7296269  53877262    153626     IND  3.586264e+04\n",
       "7296270  53877262    153626    SPER  4.707738e+04\n",
       "7296308  53863133    153629    AUTO  2.649277e+07\n",
       "\n",
       "[760 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventId       27633\n",
       "PeriodId     248280\n",
       "LobName           5\n",
       "Sum_Loss    7333970\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Perform final grouping and sorting\n",
    "final_grouped_table = final_table.group_by(['EventId', 'PeriodId', 'LobName']).aggregate([('Sum_Loss', 'sum')])\n",
    "sorted_final_table = final_grouped_table.sort_by([('Sum_Loss_sum', 'descending')])\n",
    "# The Table is now ready for the next instructions\n",
    "dataframe_1 = sorted_final_table\n",
    "dataframe_1= dataframe_1.to_pandas()\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'AGR'].empty:\n",
    "    daf_AGR = dataframe_1[dataframe_1['LobName'] == 'AGR']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'AUTO'].empty:\n",
    "    daf_AUTO = dataframe_1[dataframe_1['LobName'] == 'AUTO']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'COM'].empty:\n",
    "    daf_COM = dataframe_1[dataframe_1['LobName'] == 'COM']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'IND'].empty:\n",
    "    daf_IND = dataframe_1[dataframe_1['LobName'] == 'IND']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'SPER'].empty:\n",
    "    daf_SPER = dataframe_1[dataframe_1['LobName'] == 'SPER']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'FRST'].empty:\n",
    "    daf_FRST = dataframe_1[dataframe_1['LobName'] == 'FRST']\n",
    "\n",
    "if not dataframe_1[dataframe_1['LobName'] == 'GLH'].empty:\n",
    "    daf_GLH = dataframe_1[dataframe_1['LobName'] == 'GLH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
